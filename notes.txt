FIND STRING IN FILE:


grep -lr "plt"  
grep -Hrn 'search term' path/to/files

e.g.

grep -Hrn 'load_iris' .

LOGISTICS:

outcomes:
james@galvanize.com
ben.robart@galvize.com
email company wish list
and request for company data

EDU password: schoolhouserock
2.4: ourmembersrock!

STUDY:

pandas query
list comps for dataframes
enumeration
library itertools
iteration
key value pairs for dicionaries
negative z values
lambda with map and reduce
review z test in python

if __name__ == '__main__':

######################################################################

WEEK 1 Software Engineering + EDA

######################################################################

REPO- Assessment-day1
      pthon-intro

********************************

constructor -> creates object --> __init__ magic method
method -> function which akkcts on an object

CONCEPTS:

argmax

argument with the max value

MAGIC METHODS:

overload __name__

less than, greater than, equal __cmp__ cmp() method
allows for == < > >= <= overloading

__repr__ for returning string for printing

__init__ constructor

IPYTHON:

_ -> last resutl
_i10 -> result of line number 10
im <tab> last command that started with "im"

LISTS:

pop
extend
append

if not list_name then it is empty

ENUMERATION:

range generate all values while xrange does one at a time to save memory

for i, item in enumerate(L):
    print i, item

which is faster than a for loop

enumerates zip with izip which combines lists item by item
count is a sequence

from itertools import izip, count

for i, first, last in izip(count(), first_names, last_names):
    print i, first, last

LIST COMPREHENSION:

doubled = [item * 2 for item in L]

L = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

doubled = [[item * 2 for item in row] for row in L]

Flatten 2-D list:

L = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

[item for row in L for item in row]

[1, 2, 3, 4, 5, 6, 7, 8, 9]

********************************

REPO: sql

********************************

POSTGRES:

sudo su postgres
user postgres
psql dbname
\d # tablenames
\d tablename # column names
\q # quit

select userid, campaign_id, events.meal_id, type, 
price, event from events inner join users USING(userid) 
inner join meals ON meals.meal_id = events.meal
where event = 'bought';

select  count(events.meal_id), type

from events inner join users USING(userid) inner join meals 
ON meals.meal_id = events.meal_id where event = 'bought'
GROUP BY type;

CREATE ROLE username superuser;
CREATE ROLE username postgresql;

WITH user_campaign_counts AS (
    SELECT events.userid, count(*) as bought, campaign_id
    FROM events
    JOIN users
    ON users.userid = events.userid
    AND events.event = 'bought'
    GROUP BY events.userid, campaign_id ),

max_campaign_bought AS (
    SELECT campaign_id, max(bought) AS bought
    FROM user_campaign_counts
    GROUP BY campaign_id)

SELECT u.userid, u.campaign_id
FROM user_campaign_counts u
JOIN max_campaign_bought m
ON u.campaign_id = m.campaign_id AND
   u.bought = m.bought

Find all meals which are above average price
of previous day.

self join example:

SELECT current.dt, current.meal_id
FROM meals current
JOIN meals previous
ON previous.dt >= current.dt - 7 AND
   previous.dt <  current.dt 
GROUP BY current.dt, current.meal_id, current.price
HAVING current.price > AVG(previous.price) 
ORDER by current.dt;

******************************************************
For each user count # share events and # like events.
******************************************************

SELECT userid, 
       SUM(CASE WHEN event = 'share' THEN 1 ELSE 0 END) as share, 
       SUM(CASE WHEN event = 'like'  THEN 1 ELSE 0 END) as like 
FROM events
GROUP BY userid
ORDER BY userid;

SELECT userid, COUNT(*)
FROM events
WHERE event = 'share'
GROUP BY userid;

SELECT userid, COUNT(*)
FROM events
WHERE event = 'share'
GROUP BY userid;

SQL INTERFACE WITH PYTHON:

1- Open a connection
2- Initialize a cursor object
3- Execute a query
4- Commiting the results
5- Close the connection

import psycopg2
from datetime import datetime
conn = psycopg2.connect(dbname= 'socialmedia', host='/tmp')
c = conn.cursor()

c.execute(
    '''
    CREATE TABLE logins_7d_%ss AS 
    SELECT userid, COUNT(*) AS cnt
    FROM logins
    '''
    )


c.close()
conn.commit()
conn.close()

conn.rollback() 

c.execute(
    '''SELECT userid1
       FROM friends;'''
)

Cursor object is a generator. Each row is loaded one at a time.

l = list(c.fetchall())

c.next()

a = []

for result in c:
    a.append(result)

c.fetchall() -- returns list of tuples

********************************

REP: slq-python
     PANDAS

********************************

from math import sqrt
import random

import numpy as np
import pandas as pd
import scipy.stats as scs

PLOTTING

import matplotlib.pyplot as plt
import seaborn as sns

# ipython notebook

%matplotlib inline

pd.plot ANYTHING
sns.whatever

# ipython command line

plt.show()

# numpy data array load from csv

google = np.loadtxt('data/lunch_hour.txt')

# pandas data frame load from csv

df = pd.read_csv('data/salary_data.csv')
df.describe()

Pandas in built on numpy which is fast

ipython notebook

index 3 ways:

ix - more flexible index

loc - non-numeric index
iloc - integer index locations

boolean indexing is masking

(s > 0).head()
s[s > 0].head()

iloc[:,1:]

rows, columns

Quantitative var

histogram (order is important and bin is chosen)
kernel density estimates
normal quantile plot - for normal distribution check
Stem - leaf plot

Qualitative var

Bar chart (order is not important and bin is inherient)
Pie chart
Pareto chart is bar chart that is ordered by counts

Bivariate:

Qual & Qual - Scatter plot 
Quant & Qual  -  Box and whisker within each category side by side

Qual vs Qual: 

mosaic plot
multiple pie
stacked bar


######################################################################

WEEK 2: Statistics and Probability

######################################################################

REPO: Assessment 2
      Probability

********************************

Don't care on what you will believe if something
will happen. "confidence"

Probabity is on history

Two ways to think of frequentist probability:

1) (# of events that meet your criteria) / (total # of possible events)

2) How often does this happen in the long run ?

***********************

BAYESIAN PROBABILITY:
another way to look at probability is by Bayes Rule and 
Bayesian Probability:

Fundamental belief - not strictly data - that something is true not changed
by history. "chance","likely hood" , "previous" or prior, 

"Prior Belief" is essential to Bayes 

prior - probability distribution of one's beliefs before collecting any data
        equal weight to all called "uninformed prior"
        Posterior becomes prior for next round 

likelihood - Probability of data given some belief / outcome 

Posterior - probability distribution of one's belief after collecting data
            Multiply prior * likelihood of given data value

Conjugate prior - if the posterior and prior are from the same family of 
probability distribution, the are conjugate distributions, and the 
prior is a conjugate prior to the likelihood

Define two events A and B

Bayes Rule:

P(A | B ) = P(A) * (P( B | A)) / P(B)

Mutually exclusive vs. Independent events
(rain, no rain - cannot occur at the same time)

Independent Events: P(A and B) = P(A)*P(B)

Rewriting using Law of Total Probability:

P(A- | B ) = P(B|A)* P(A) /( P(B|A) * P(A) + P(B| A') * P(A'))

2)

*******************************************

one set of events is called marginal probabilies
multiple sets of events is called joint probabilies

*******************************************

Random variable is a capital letter.
specific value is a lower letter

~ is notation for distributed as:

X ~ Normal(mean, variance ** 2)

*******************************************

Four parts important for distributions:

1) Measure of center (median, mean, mode)
2) Measure of spread (5# summary (min, max, q1, q2, q3) , variance, range, std. dev)
3) Shape (skewed, symmetric)
4) Outliers

*******************************************

CDF -> Cummulative Distribution Function
PMF -> Discrete or qualitative probability mass function
PDF -> Continuous of quantitative probability density function (CDF derivitive)

*******************************************

Expectation or Expected value is the weighted average

Sum up each value * probability of each value

*******************************************

Variance - measure of the variablity around x

Var(x) = Expected_value(x ** 2) - (Expected_value(x)) ** 2
       = Expected_value(( x - Expected_value(x)) **2 )

Expected_value(x**2) = sum_of ( x**2 * p(x) )

Standard Deviation is the square root of variance in order to compare units

Discrete random variables

*******************************************
HOW TO MODEL
*******************************************

1. Identify your random variable (R.V. X or Y e.g. height)
   (quantitative, qualitative, start min, end max values, continuous, discrete)
2. Identify the distribution
3. Identify the parameters
4. Answer the question of interest

*******************************************
DISTRIBUTIONS
*******************************************

Bernoulli ( p ):  one if a coin with heads probability
        p comes up heads, zero otherwise. (one coin flip)

Binomial ( n, m, p) : the number of heads in n independent flips of a coin with heads probability p .
Only takes on two events. Sum of Bernoulli. (multiple coin flips)

(n x) = n! / (x! ( n-x) !)

Expected value = n * P

Geometric (p) (where p > 0): the number of flips of a coin with heads probability 
                             p until the first heads
            binomial distribution with number of success until failure
            e.g. 95% free throw shooting percentage

X = makes of shots - binomial
X = makes of shots until first miss- geometric
X = make 8 shots - with a 95% rate - binomial
scs.binom.pmf(8, 10, .95)

Negative Binomial - geometric combined until n misses e.g. trial until n (e.g.) 3 misses

Poisson (lambda) (where lambda > 0): a probability distribution over the nonnegative integers 
used for modeling the frequency of rare events
Number of events that can happen in an interval.
Positive discrete. Right skew. Non-symetric. 

lambda = mean = variance

Continuous random variables:

Uniform (a,b) (where a < b):  equal probability density to every value between a and b on the real line.

******************

Beta ( alpha, beta ) continuous between 0 and 1 - generally for proportions

Beta(alpha, beta)  = c ** ( alpha - 1) * ( 1 - c ) ** (beta - 1) / ( beta(alpha, beta) )

c = click through rate / conversion rate

Beta ( 1, 1) is not any information
Beta ( 500, 500) this gives enough samples for konwing what is going on

Gamma( alpha, beta ) 
expected value = alpha * beta
variance = alpha * beta ** 2
Right skewed continuous distributions .  Range 0 to positive infinity.

special cases of Gamma:

******************

Chi Squared ( degrees of freedom )  : independece test of two 
categorical variables  beta = 2 alpha = df/2
or ... Gamma( df /2 , 2 )
also used for comparing proportions

df = (nrows - 1) * (ncols - 1)

Used to compare two categorical groups for independence

Test your variance equal to some value

p value is always shaded to the right

H0 Independent
H1 Not Independent of two categories

******************

standard normal 0, 1

Exponential (lambda) : decaying probability density over the nonnegative reals

******************

F-distribution (dfn, dfd)
t-distribuion (df)

******************

Normal (mean, standard deviation ** 2) :  also known as the Gaussian distribution 
variance = standard deviation **2
Model over entire real line
min = negative infinity
max = positive infinity
expected value is the mean
--> good for averages and porptions

Gaussian random variables are extremely useful in machine learning and statis-
tics for two main reasons.  First, they are extremely common when modeling 
“noise” in statistical algorithms. Quite often, noise can be considered 
to be the accumulation of a large number of small independent random 
perturbations affecting the measurement process; by the Central Limit 
Theorem, summations of independent random variables will tend to 
“look Gaussian.” Second, Gaussian random variables are convenient for 
many analytical manipulations, because many of the integrals involving 
Gaussian distributions that arise in practice have simple closed form 
solutions.  We will encounter this later in the course

For any continuous distribution no probability for a single point.

******************
SCIPY
******************

import scipy.stats as scs 

# for binomial distribution

scs.binom.pmf(x, n, p)

# X = make 8 shots out of 10 - with a 95% rate - binomial

scs.binom.pmf(8, 10, .95)

# X = make at least 8 shots out of 10 - with a 95% rate - binomial
# take results minus 1 to get the part of the chart

1 - scs.binom.cdf(7, 10, .95)

******************

4 things to get back:

scs.nameofdistribution. and then...

1 - pmf - P(X = x) discrete qualitative
          probability mass function value at instance 

* - pdf - P(X = x) continuous - for plotting not solutions - so not counted for use
          probability density function (quantitative continuous) should be 0

2 - cdf - P(X <= x) may need to subtract from 1
          Cummulative Distribution Function under a range P(X <= x)
3 - ppf - returns x given probability (quantile) "backward calculation"
          P( X <= x) for discrete and continuous e.g. how many shots in 85% of the time?
4 - rvs - size for random draws for the distribution

******************

Estimation Techniques for Distribution Parameters

******************

1 - Maximum Likelihood Estimations (MLES)
2 - Method of Moments (MOMs)
~ 3 - Maximum a Posterior (MAPs)

MLES is Calculus Based:

Asymptopic statistical properties are with large sample sizes

Calculate:

1) compute joint distribution (likelihood)
2) log the joint distribution
3) take the derivative with respect to your parameters
4) set to 0
5) solve for parameters

Example Poisson distribution:

1) f(x|lambda) = lambda ** x e ** -x / x!

2) likelihood ( lambda | x_sub_i ) which is proportional to the log 
sum ( x_sub_i * log(lambda) - lambda * n )

5) lambda = mean

******************

Easiest is MOMs which is Algebra Based:

1) take expected value and equate it to a moment

First and second moments for all distributions:

E[X] = 1/n sum x_sub_i = M1
E[X**2] = 1/n sum x_sub_i ** 2 = M2

variants for all distributions:

VAR[X] = E[X**2] - (E[x]) ** 2

e.g. for a normal distribution:

VAR[X] =  sigma ** 2

e.g. for a beta distribution:

X ~ Beta ( alpha , beta)

Expected value:

E[X] = alpha / (alpha + beta)

VAR[X] = alpha * beta / ( alpha + beta ) **2 (alpha + beta + 1 ) )

Calculate M1 and M2 from values using moment calculations:

M1 = alpha / (alpha + beta)
M2 - M1 ** 2 = alpha * beta / ( alpha + beta ) **2 (alpha + beta + 1 ) )

********************************

REPO: Sampling and Estimation

********************************

Populations/Parameter/Sample/Statistic

******************

Statistic is numeric from sample (lowercase symbol)
Parameter is numeric from population (Greek symbol)

******************

Central Limit Theorem

******************

the statistics distribution are normally distrution will
be normal with a large enough sample size. 30 is a good 
guess

******************

Sampling Distributions

******************

Distribution of a statistic. 
X ~ N(mean, (variation/sqrt(n))**2)

Example:

Consider I am interested in every
person at Galvanize's average
commute.

I select 50 people at
random from the 
building.

I find an average commute of 35 min
with a standard deviation of 20 minutes.

DO FIRST:

Population: Everyone at Galvanize
Parameter: The average time to commute (mue)
Sample: 50 people selected 
Statistic: x = 35 min s = 20 min

x is a normal distributed from the central
limit theorem

How to determine the average travel time for 
everyone? 35 +- 2 deviations

******************

CONFIDENCE INTERVALS

******************

Only good for sample statistics

t* - is a t* distribution for a sample
by degrees of freedom

x +- t* sub (n - 1) * (s / sqrt(n))

Not really used but only if you large sample
size

Z* - is a Z* population distribution

x +- Z* (s / sqrt(n))

95% confidence of data in the middle of the curve.

35 +- scs.t.ppf(.975, 49) * 20 / sqrt( 50 )

scs.t.pdf(.975, 49)

1 - 0.975  = 0.025 

+- ----> give 5%

area on the left 
50 sample size - 1

We are 95% confident the average commute for all 
Galvenize travelers is between ( and ).

*use confident instead of chance

******************

Bootstrapping

******************

Repeated samples from samples space.

Treat sample as a population.


********************************

REPO: Frequentist Hypothesis Testing

********************************

estimation approaches

- maximum likelihood estimation (central limit applies)
- moments

all assume normal distribution

Null hypothesis H0 is what you assume to be true before collect any data.

1. H0 alternative hypothesis H1
2. Figure out what test. Calculate some test statistic.
common z (proportions), t (means), chi squared, F tests, 
Welches) from a distribution
3. Obtain p-value
4. p-value <= alpha then reject null hypothesis
(determines whether or not alternative H1 is accepted)
5. Conclusion in terms of your problem.

p-value is greater than alpha when you reject the null hypothesis

Power = 1 - beta = of the test is the probability to choose H1 correctly

Parachute Example:

type 1 error: Worst type of error - alpha  - think the parachute 
will open but it does not. Chose H1 when H0 is really true

scs.norm.ppf(0.95, 0, 1) z score

type 2 error: beta - think the parachute will not open but it does
Chose H0 when H1 is really true

cdf to get area under curve

how to calculate the value of beta...

effect size is the difference between two means

Hypothesis Testing Example

Consider I believe the average commute time to
galavanize is 30 minutes or less for all 
individuals

Null hypothesis H0 is what you assume to be true before collect any data.
Alternative hypothesis H1 is what you want to test that you think is true. 

n = 50
mean = 35
std = 20

H0: m >= 30
H1: m < 30

use t statistic for a single test

t = (est - value H0/ SE est)

df = n-1

H0 t = (35 - 30)/ 20/sqrt(50) = 1.77

scs.t.ppf(0.975, 49) 

3 - 1.77 value p value shaded to the left of 1.77 because of less than

if scs.t.cdf(1.77, 49)  > .05 then failed to reject null hypothesis

conversely if null hypothesis is < 30 then :

if 1 - scs.t.cdf(1.77, 49)  < .05 then reject the null hypothesis

******************

for two means use paired t test 

df = n-2 for 2 means

for two means use unpaired t test 
e.g. Welches test

(x1 - x2)  - (mean1 - mean2) / (SE (x - x2)

df = n-2 for 2 means

for test proportion normal z or for values between 0 - 1

for compare two proportions normal z or for values between 0 - 1

one sample proportion 

df = n-2 for 2 means

proportion has z test
means has t test

******************

Publishing bias 

high bias refers to the error introduced by a simple model
bias to the model not the data
More flexible models lower bias

multiple comparison techniques

5 publish can have type 1 errors
while 95 cannot publish that do not have type 1 errors

Bonferroni Correction: new alpha = old_alph / number_tests

******************

p-value

The probability of getting your data or more extreme data
if the null is true

P(x | H0 is true) 

******************

When you have really large sample sizes everything is statistically
significant

Need to use something other than a p value.

Confidence interval can also be used as a hypothesis test.

Know what the result of your test is "saying"

e.g.

H0: m1 = m2    
H0: m1 - m2 = 0

H1: m1 != m2

Shade the alternative hypothesis

******************

Experiments vs observational studies

Experiments assign individuals to a treatment


********************************

REPO: Power Calculation and Bayes

********************************

Bayes Rules

https://en.wikipedia.org/wiki/Conjugate_prior

Conjugate priors are nice! Beginning belief of the distribution.
Posterior has the same distribution

1. Simple Example

Data follows a Poisson distribution

Conjugate prior is a gamma distribution because is goes from 0 to infinity
with the same range as the distribution of the MEAN. Feeding in 
distribution of the parameter of the likelihood of the data.
right skewed is gama

Gamma will be the posterior with the gamma conjugate prior to generate 
the mean for the distribution alpha and beta

exponential is a special case of gamma distribution where 
beta is not fixed and alpha is 1

chi squared is a special case of gamma for 
categorical modeling and variance variance modeling

t distribution has degrees of freedom as a parameter

Keep updating conjugate prior with new posterior hyperparameters 
using the gamma distribution 

X ~ Poisson(lambda)

Likihood(x|lambda) ~ Poisson(lambda) <- Data generator
Conjugate Prior - Pi(lambda) ~ Gamma( alpha_0, beta_0) <- Hyper Parameters

lambda is a draw from the conjugate prior

Postierer parameters -> Pi(lambda | x) proportional_to  
Gamma(sum(x_i) + alpha_0, ( n + (1/beta_0) ) -1 )

2. Advanced Example

******************

Bayes using Beta Click through

******************

Objectives:

1- Describe what each of the three parts of the Bayesian learning framework represent
2- Define what a conjugate prior is and why it is useful
3- Describe why the beta-binaormial conjugate pair / likelihood is useful in online learning
4- Define A/B testing
 
Prior: Beta( alpha, beta) for c - click through rate/conversion rate

Likelihood: Binomial( n, c ) = ( n k ) * c **k (1 - c) ** (n - k)  n is number of trials - k is number of successes

e.g. n is number of trials and k is number of click successes e.g. conversions

Postier: New parameters: alpha = alpha + k 
                         beta = beta - k + n

Beta(alpha, beta)  = c ** ( alpha - 1) * ( 1 - c ) ** (beta - 1) / ( beta(alpha, beta) )
Likelihood: Binomial( n, c ) = ( n k ) * c **k (1 - c) ** (n - k)  n is number of trials - k is number of successes

likelihood*prior -> proportional to 

Start with alpha = beta = 1 for uniform prior

first visit is 1 success

alpha = 1 + 1 = 2
beta  = 1 + 1 -1 = 1

next visit is 0 success

alpha = 2 + 0 = 2
beta  = 1 + 1 - 0 = 2

alpha = 3
beta = 2

a = 3 b = 3 or 50% success

Bayes using Beta Click through

********************************

REPO: Multi-armed-bandit

********************************

maximize returns on a slot machine

try and explore multiple slot machines

trade off between exploration and explotation

regret is a meaure of how often you choose a sub-optimal bandit
epsilon first is some exploration and more exploitation

bernoulli trial is yes/no on return beta distributions
baysian update process

start with prior of uniform distribution

Baysian does not have confidence and hyposisis testing 
Frequentist has 95% confidence interval and hyposisis testing 

Frequentists is looking for data that follow a "true" parameters
Prob(Data given parameters)
Baysian is looking "true" parameters givne data
Prob( parameters given data)

######################################################################

WEEK 3: REGRESSION

######################################################################

REPO: Linear Algebra, EDA, Linear Regression

********************************

dot product of two vectors is a scalor or the covariance

euclidean(a ,b) is the distance between two points or vectors

vector norm is a number

cosine_similarity can be used instead of
euclidean(a ,b)

np.corrcoef(a,b)[0,1]

vector_norm(a) - length of the vector from the origin

also called lasso penalty
lambda parameter

Lambda increasing results in the model becoming more rigid which reduces variance and increasing bias
Lambda decreasing results in the model becoming more complex which increases variance and decreases bias

np.sum([np.absolute(x) for x in a])
np.sum([ x** 2 for x in a])

v = np.random.randint(10, size=4)
v
w = np.random.randint(10, size=4)
w

A = np.random.randint(10, size = 8).reshape(2,4)
A.transpose()

Matrix multiplication is the Dot product. 

mat1 = np.random.randn(12).reshape(4, 3)
mat2 = np.random.randn(15).reshape(3, 5)

a = np.arrange(1,7).reshape(3,2)
b = a.T

in general: A r1 xc1 B r2 xc2

c1 = r2

A dot B = r x c2

A = [[ 1, 2 ],
     [3 , 4], 
     [5 , 6]] 

A.T = [[ 1, 3, 5 ],
       [ 2, 4, 6 ]]

A dot A.t = [[5, 11, 17],
       [ 11, 25, 39 ] ,
       [ 17, 39, 61 ]]

1*2  + 

1*5 + 6* 2 = 17

Inverse of a matrix needs to be a square matrix.

such that the dot product of A and A -1 is equal to the identity matrix

A prime = A transpose

Zero vector = a vector of zeros

Norm is 0 of a zero vector

The vector {3, 4} norm is 5. sqrt( 3**2 + 4**2 ) = 5

The vector {1, 1, 1, 1} = sqrt ( 1 + 1 + 1 + 1 ) = 2

Shape of A 1xn DOT B 1xn  shape 1 x n
A (T) 5X4 shape 4X5

A 10 X 3 dot B 3 X 5 shape 10 X 5

A 3 X 10 dot B 5 X 3 CANNOT BE DONE

(A 5x4 dot B 4X3 ) -1 CANNOT BE DONE

Given A 5X4 , ( A T A ) -1 

A T is 4X5

(4X5 dot 5X4) -1 = > (4 X 4) square so can take inverse

inverse of a (4X4) is (4X4)

Beta = (X tranpose dot X ) -1 dot X transpose y

Dot product of 2 products is a scalor

********************************

REPO: Linear REGRESSION 2

********************************

y = beta0 + beta1(treatment) + E + beta2(age)

look at difference of means between beta0 and beta1 for the treatment
using t test

beta0 is the y interecept 
beta1 is the y interecept 

same data with notation for whether or not treatment is done

BP TR AGE

100 0 25
110 1 30
120 0 40

for regression the distribution of y is contingent on x

Instead of looking at:

y ~ N( mean, deviation)

Linear regression tests for:

y ~ N( x * beta, omega * T)

beta is the slope of the regression line

Regression is used for:

1. Inference

look at coefficents and distributions and check the p value for the mean
for significance

2. Predictions y hat values

RSS is the residual of the sum of squared errors

MSE = 1/N * RSS which is the root mean square error

FMSE = sqrt(MSE) which puts us back on scale

RSE = sqrt( 1 / (n-p-1) * RSS )

residual standard error which accounts for the degrees of freedom

R**2 is a good way to compare different models with different
measurements

R**2 = (TSS - RSS)/ TSS = 1 - RSS/TSS

TSS = summation ( y_i - y_bar) ** 2
RSS = summation ( y_i - y_hat) ** 2

y_bar = mean(y)
y_hat = corresponding y value on estimated line

********************************

REPO: Regularized REGRESSION

Lasso and Ridge Validation

********************************

Robust Residual 

RSS = sum(abs(y - y_hat))

iid = Independent and identically distributed 

errors are iid normally distributed

Studentized residuals

E_i iid N(0, sigma**2)

Need to test normality of this error

Can check:

********************************

REPO: Linear REGRESSION 2
lecture.pdf

********************************

- Normal Q-Q or QQ plot, histogram
Check for normal distribution using QQ plot

Plot studentized residuals to see if they are normal 
against theoretical quantiles and should fall on a line

"Alligator chart" of non-linear residuals.

heteroscedasticity

To address non-linear residuals plot the studentized residuals
computed by dividing each residual e_i by its estimated
standard error. Observations whose studentized residuals
are greater than 3 in absolute value are possible outliers.

line up sample data with values in a normal curve
n + 1 buckets mapped on normal distribution.
Order values from lowest to hightest and scatter plot
against normal distribution.

Normal Q-Q Plot

import scipy.stats as scs
import matplotlib.pyplot as plt

y_values = sorted([4.75, 7.90, 7.21, 5.80, 6.33, 5.78, 5.20, 4.75, 3.89])
n = len(y_values)
z_values =  [[scs.norm.ppf(float(i)/(n+1)) for i in xrange(1,n+1) ]]

plt.scatter(z_values, y_values)
plt.show()

Studentized Residuals should be N(0,1)

Local regression - use sliding weight function, make separate linear fits over range X

Generalized additive model - just add up contributing effects

Need to check for normal error_i  is a normal distribution
it is proportional to studentized residials ~ N(0,1)

Need to check that the error rate is sigma**2

Solution is to look at y - because it is used to calculate error
adding more components will not help.

****************************************************************

Multiple Linear Regression:

More x's available to predict y

1) Constant Variance (homoscedasticity)
2) Linear model is approriate
3) Normally distribution of the errors 
4) iid  - Independent and identically distributed
5) no multicollineartity - or coorelation between any variables

multicollinearity - 

Can check by VIF variance inflation factors

1/ (1 - R**2)
Large values indicate covariance

- Correlation Matrix / Scatterprlot Matrix

- Variance Inflation Factors (VIF)

-- Run ordinary least squares for each predictor as function of 
all the other predictors . k times for f predictors

Rule of Thumb > 10 is problematic

****************************************************************

OUTLIERS:

occur when y_i is far from predicted y_i_hat (estimated)

find out why it is an outlier before removing

Correlation is a linear relationship between factors

Dividing each residual by its standard error,
should result in a "studentized residual" between -2 and 2
for 2 standard deviations
as a rule of thumb

LEVERAGE

Leverage point : Var(e_i) = (1 - h_ii) * sigma ** 2

Beta = (X.T * X) ** -1 X.t * y

XBeta = y_hat = H * y

H = X ( X.T * X) ** -1 X.T

h_ii = (H)_ii

some points affect the regression more than other points

Most influential points are outliers with high leverage (far from 
cluster of points

Look for interaction between variables by seeing systematic
under or over predicion of values using heat map

p value should be LESS than 0.05 for regression results

Variance - in terms of fitting a model - fluctuations 
in model due to change in our training data

Bias - error introduced by modeling complex data with
simplistic model - how far off the expected value is from
the true model

Strike a balance between variance and bias

- 100 x variables need to decide on which 20 variables to
use look at p values to see what to remove
after fitting all variables

- look for redundant variables or classified together and 
choose one

- business choice for predictive variables

Coefficent size is in proportion of the x variable so not 
relative to each other

**************

BEST SUBSET

**************

Fit all possible models - computation intensive so not
practical

**************

FORWARD SELECTION

**************

Fit it with each variable alone
Pick winner with highest RSS

Pick winner with combo of the rest of the variables

**************

BACKWARD SELECTION

**************

Fit with all variables
Remove on variable at a time and compare results

**************

Drawbacks of R**2 / RSS and why measures like
AIC, BIC, and adjusted R**2 are more approriate.

R**2 is easy to understand

R**2 is localized to the training data so it
is not reflective of what it may be like with
a new data set

MEDIOCRE TESTS:

R**2 RSS

R**2 increases or stays the same
with more variables and RSS goes down or stays the
same

BETTER TESTS:

Adjusted R**2 is better - higher is better = 1 - RSS/(n-p-1)
                                                ____________
                                                 TSS/(n-1)
AIC - lower is better = -2ln(L) + 2k
BIC - lower is better = -2ln(L) + ln(n/k)

Introduce penalty term for complexity

k - number of predictors
n - sample size or number of observations
L - likelihood
p - number of predictors

BEST TEST:

MSS on a cross validation data set

******************************************************

TEST FINAL MODEL FOR BEST MODEL COMPARISON
USE VALIDATION

******************************************************

*********************

Validation Set Approach
----------> Not a good approach

all data is used for generating models to be
compared against each other

1. Fit model on training set
2. Use fitted model in 1 to predict responses for validation set
3. Compute validation-set error

*********************

Hold out data for second test for new error metric on a new
data set.

Validation Set Approach:

randomly pull training and validation set from the entire
data set

Validation training set is used to calculate the MSE

Randomly keep pulling sets out of the data and recalculate
MSE 

*********************

Cross Validation Set Approach

test set < 50% - model never sees it 

training set is used for cross validation
never used for creating the model

Don't take random subsets
Take a predetermined subsets

*********************
Leave one out cross validation
*********************

For each model:

    1 - fit same model n times
    2 - method 
     for n times within the training set:

         a - take n - 1 data points
         b - fit the model on n - 1 data points
         c - predict the n'th data point that was left out
         d - calculate error
        
    3 - MSE of leave one out cross validation

*********************
k-fold validation

----------> Best validation

*********************

Pick the number of folds between 5 and 10 not at random

1. Fit model on training set, using (k-1) folds
2. Use fitted model in 1 to predict responses for 
validation set, 1 of the folds
3. Compute validation-set error

For each model:

    1 - fit same model k fold times (training set divided k times)
    2 - method 
     for k times within the training set with the folds:

         a - leave out k fold for training
         b - fit the model on the remaining folds
         c - predict the fold that was left out
         d - calculate error
        
    3 - MSE of k fold cross validation

******************************************************

Comparison of models

BIAS

LOOCV < k-fold < validation set

VARIANCE 

validations set < k-fold < LOOCV

******************************************************

After training and validating data then use all the
data before deploying model using training and test 
set data to get the final parameters

******************************************************

Bias-Variance Tradeoff

******************************************************

Variance is high complexity . Amount by which f_hat 
would changed if estimated it using a different training
dataset. Often overfitting problem.

Bias difference between expected prediction of our
model and the correct value we are trying to predict
Measure of the prediction of the correct value.

VARIANCE - as complexity reduces variance reduces
BIAS - as complexity increases bias increases

******************************************************
Regularization Ridge Regression

lambda is a tuning parameter

Regularization lasso Regression

when lambda = 0 we have linear models

lambda is a tuning parameter

L1 is the least absolute values
L2 is the least squares

Standardize the predictors by dividing by the
standard deviation

Need to do this anytime that you need to 
penalize beta

********************************

REPO: Assessment-3 
      Logistic =-regression

********************************

Motivating problem:

[Binary] Claassification

Response Y is an element of {0, 1}

which is mutually exclusive

Goals:

1 - Estimate
2 - Interpret
3 - Predict
4 - Metrics

Preliminary:

What Distribution?

Bernolli takes p parameter

PMF:

Expected Value ( y ) = p if y = 1
                     1-p if y = 0

LIKELIHOOD function: or expected value

E(y) = p**y ( 1 - p) ** (1-y)

Variance:

p

OLS is a straight line

which is not a good fit for logistic regression 

OLS does deal well with separation

separation is the strength of a factor in the regression

Variance of OLS

errors are contigent on x
x *beta ( 1 - x *beta) 
if x is continuous p becomes x and becomes continuous

errors are heteroscatic which is a violation of normality

Need link function for logistic regression:

1 / ( 1 + exp(-X beta ))

Likelihood for all the observations in the data 

First step to maximize parameters is to change the products to
logs.

Instead work with log liklihood and take the natural log:

which turns the product into a sum of values.

Log likelihood of values between 0 and 1 (probablilty) is a
negative number. so take the negative value.

Penalty term

L1 = sum of absolute values of beta 
L2 = sum of beta squared

Penalize likelihood by subtracting penalty term

lnL = blabla_summing - [L1, L2]

Biased coin:

P(H) = (0.75)
P(T) = (1.0 - 0.75) = 0.25

Probability event for odds of heads:

0.75/ 0.25 = 3 

or 3 to 1

ODDS are :

p / (1-p)

take log of this value will yield :

X * beta

Get back odds from skilear by :

exp( beta )

predicted probabilities 

********************************

LOGISTIC METRICS

********************************

Need to classify predicted events as a classified event
0 or 1 using a threshold. Default is 0.5 . Above 0.5 is 
classified as an event otherwise not an event (zero).

True positives are above the threshold

Lower Thresholds have more false positive Type I error
Higher Thresholds have more false negatives Type II error

True negative are below the threshold

Partitioning into quardrents is called a confusion 
matrix to show what is correct and what is not.

1. Accuracy = skilearn score = all correct / all cases

2. True Positive Rate = TPR = true_positives / over positive cases
Recall or sensitiviy

3. False Positive Rate = FPR = false_positives / N
1 - specifity

4. True Negative Rate = TPR = true_negatives / over negative cases
specifity

5. Precision = true positives / (true positives + false positives)

Increasing the threshold will:

:grinning: decrease the number of False Positives,
:weary: decrease the number of True Positives,
:weary: increase the number of False Negatives, and
:grinning: increase the number of True Negatives.

ROC:

function ROC_curve(probabilities, labels):
    Sort instances by their prediction strength (the probabilities)
    For every instance in increasing order of probability:
        Set the threshold to be the probability
        Set everything above the threshold to the positive class
        Calculate the True Positive Rate (aka sensitivity or recall)
        Calculate the False Positive Rate (1 - specificity)

Area under ROC should be closest to 1 for ideal values

ROC curve should not below diagnol line

Accuracy may be a bad example - 

********************************

REPO: GRADIANT-DESCENT

********************************

What: Numerical Optimization

Why: minimize a a cost function such as MSE (descent)
or maximize a log likelihood function (ascent)

e.g.  E(y) = p**y ( 1 - p) ** (1-y)

pi = 1 / ( 1 + e ** -x*beta)

Gradient is the partial derivative. Positive slope is going up
Negative slope is going down.

Zero value is where the slope changes direction

hessian is the second derivative and is reflective of the 
standard error. 

Tighter curves have smaller hessians than wider curves

Step size of riding the curve is called "alpha"

while lnL increasing:

1/beta = 1/beta + ( alpha * gradient )

Set a threshhold for the value close enough to zero.

Option Criteria:

1) derivative of beta ( ln L ) ~ 0
2) cap for number of iterations (max_iter)

Two Types of Gradient Ascent:

1) Batch Gradient Ascent:

set beta_k = 0

while lnL increasing on full training data:

beta = beta + ( alpha * gradient)

2) Stochastic Gradient Descent:

set beta_k = 0

row by row
while ln L increasing:
    for i to N:
        beta = beta + ( alpha * G)

Regression Review:

Simple Linear Regression
Assumptions

y ~ N(x Beta , sigma ** 2)

y_hat = beta_0 + beta_1 * x_1

1) Constant Variance
2) Linear model is approriate
3) Normally distribution of the errors 
4) iid  - Indenpendent and identically distributed

There are four principal assumptions which justify the use of linear regression 
models for purposes of inference or prediction:

(i) linearity and additivity of the relationship between dependent and independent 
variables:

    (a) The expected value of dependent variable is a straight-line function of 
    each independent variable, holding the others fixed.

    (b) The slope of that line does not depend on the values of the other variables.

    (c)  The effects of different independent variables on the expected value of the 
    dependent variable are additive.

(ii) statistical independence of the errors (in particular, no correlation between 
consecutive errors in the case of time series data)

(iii) homoscedasticity (constant variance) of the errors

    (a) versus time (in the case of time series data)

    (b) versus the predictions

    (c) versus any independent variable

(iv) normality of the error distribution.

If any of these assumptions is violated (i.e., if there are nonlinear relationships 
between dependent and independent variables or the errors exhibit correlation, 
heteroscedasticity, or non-normality), then the forecasts, confidence intervals, 
and scientific insights yielded by a regression model may be (at best) inefficient 
or (at worst) seriously biased or misleading.  More details of these assumptions, 
and the justification for them (or not) in particular cases, is given on the 
introduction to regression page.  one unit in change with beta coefficient 
changes y by coefficient in regression holding everything else constant.

Add dummy variables for categorical variables in regression
Add levels - 1 for number of dummy variables.

Diagnostics - compare linear regression models - MSE , AIC,  
adjusted R**2, BIC

Check residual plot for residuals vs. independent variables

k folds to compare models with predictive power

Inapproriate measures - MSE of training set the one with more variables with have a better MSE

######################################################################

WEEK 4 SUPERVISED LEARNING

######################################################################

REPO: Non Parametric Learners

********************************

Generalized Linear Models (GLMs) are parametric
Regression is parametric with normal assumptions
Response values - continuous (linear regression) vs. binary for logistic
Supervised - has predictive label (linear and logistic regression) vs 
Unsupervised - no predictive labels  - looking at relationships
Parameters of Interest
Predictor values
Bias-variance trade off - linear model high bias and low variance 
Bias-variance trade off - decision trees have model low bias and 
high variance 
Advantage of Regression is that it has interpretable beta coefficients

recursion. 

The 3 laws of recursion algorithm:

1. must have a base case.
2. must change its state and move toward the base case.
3. must call itself, recursively.

k-nearest neighbors (kNN) - high bias approach
k nearest neighbors (kNN) - high bias approach

Parameters 1- how many neighbors 2- neighbors weight

k should be an odd number so no ties

Low neighbors (k) has low bias and high variance
High neighbors (k) as high bias and low variance as averaging more values

Library sklearn.neighbors.KNeighborsClassifier

Response Values - Mostly uses categorical values
Predictor Values - Categorical or Numerical

Measures of "closeness" :
- Cosine similarity - e.g. good with text data for scaling from 3 to 300
because on the same line (3,4) is on the same line as (300, 400)
- Euclidean Distance - e.g. not good with scaling (3,4) is not close to
(300, 400)

Pros: High accuracy, insensitive to outliers, no assumptions about data
good for intuitive algorithm,  looking for missing values, not used 
often for prediction
Cons: Computationally expensive, requires a lot of memory
computational intensive if lots of dimensions
curse of dimensionality

Works with: Numeric values, nominal values

1. Collect: Any method.
2. Prepare: Numeric values are needed for a distance calculation. 
A structured data format is best.
3. Analyze: Any method.
4. Train: Does not apply to the kNN algorithm.
5. Test: Calculate the error rate.
6. Use: This application needs to get some input data and output structured 
numeric values. Next, the application runs the kNN algorithm on this 
input data and determines which class the input data should belong to. 
The application then takes some action on the calculated class.

For every point in our dataset:
calculate the distance between inX and the current point
sort the distances in increasing order
take k items with lowest distances to inX
find the majority class among these items
return the majority class as our prediction for the class of inX

Decision Trees

Pros: Computationally cheap to use, easy for humans to understand 
learned results, missing values OK, can deal with irrelevant features
Cons: Prone to overfitting
Works with: Numeric values, nominal values


1. Collect: Any method.
2. Prepare: This tree-building algorithm works only on nominal values, 
so any continuous values will need to be quantized.
3. Analyze: Any method. You should visually inspect the tree 
after it is built.
4. Train: Construct a tree data structure.
5. Test: Calculate the error rate with the learned tree.
6. Use: This can be used in any supervised learning task. 
Often, trees are used to better understand the data.

ID3 algorithm

http://en.wikipedia.org/wiki/ID3 _algorithm

Using information theory, you can measure the information before and 
after the split.

The measure of information of a set is known as the Shannon entropy, 
or just entropy for short.

To calculate entropy, you need the expected value of all the information 
of all possible values of our class.

CLASSIFICATION Tree USES Gini coefficient or cross entropy.
sklearn defaults to Gini
Gini uses p for proportion 
can be at split or for entire model

sklearn.tree.DecisionTreeClassifier

sklearn.tree.DecisionTreeRegressor

negative mean square error is reported by sklearn 

REGRESSION Tree uses RSS

sklearn fits to single node which is overfitting by default
give it the minimum number of samples or max depth

Pruning will prevent overfitting by chopping off lower branches
with high misclassification rates

Make a split for node purity to increase


Another common measure of disorder in a set is the Gini impurity.

Assumptions about data for trees.py:

1) Data comes in the form of a list of lists 
2) all these lists are of equal size 
3) last column in the data or the last item in each instance is the 
class label of that instance 

You use these assumptions in the first line of the function to find 
out how many features you have available 
in the given dataset. 

We didn’t make any assumption on the type of data 
in the lists. It could be a number or a string; it doesn’t matter.

Decision trees can be applied to both regression and classification problems

CLASSIFICATION TREE - for categorical response values

REGRESSION for quantitative continuous response data

REGRESSION

Building a Regression Tree
1. Use recursive binary splitting to grow a large tree on the training
data, stopping only when each terminal node has fewer than some
minimum number of observations.
2. Apply cost complexity pruning to the large tree in order to obtain a
sequence of best subtrees, as a function of α .
3. Use K-fold cross-validation to choose α . That is, divide the training
observations into K folds. For each k =1 ,...,K :
(a) Repeat Steps 1 and 2 on all but the k th fold of the training data.
(b) Evaluate the mean squared prediction error on the data in the
left-out k th fold, as a function of α .

Average the results for each value of α , and pick α to minimize the
average error.
4. Return the subtree from Step 2 that corresponds to the chosen value

********************************

REPO: Random Forest

********************************

CLASSIFICATION

A classification tree is very similar to a regression tree, 
except that it is classification tree
used to predict a qualitative response rather than a quantitative one.

We predict that each observation belongs to the
most commonly occurring class of training
observations in the region to which it belongs.

class proportions among the training observations that fall into that region

Decision trees for regression and classification have a number of 
advantages over the more classical approaches such as regression:

1) Trees are very easier to explain than linear regression

2) decision trees more closely mirror human decision-making 
than do the regression and classification approaches

3) Trees can be displayed graphically, and are easily interpreted 

4) Trees can easily handle qualitative predictors without the need to
create dummy variables.

Disadvantage

---> Not good predictors 
---> High variance

Bagging, random forests, and boosting use trees as building blocks to
construct more powerful prediction models 

combining a large number of trees can often result in dramatic
improvements in prediction accuracy, at the expense of some 
loss in interpretation.

bagging - bootstrap aggregation - averaging a set of observations 
reduces variance

samples from the (single) training data set. In this approach we generate 
B different bootstrapped training data sets. 
We train our method on the b th bootstrapped training set:

f_hat ∗ b (x), and finally average all the predictions, to obtain

f_hat_bag (x)= 1 B B b =1 f_hat ∗ b (x) This is called bagging.

random forests - in building a random forest, at each split in the tree,
the algorithm is not even allowed to consider a majority of the available
predictors. Take random sqrt(number of features) for each tree split
This would decorrelate the tree and would keep the trees looking the
same.

can distribute easily random forests across parallel processors

every tree has roughly 2/3 of all data

trees grown out tree reduced bias and high variance deep trees
average across the trees reduce variance across them

The main difference between bagging and random forests is the choice
of predictor subset size m .

boosting

boosting works like bagging, except that the trees are
grown sequentially : each tree is grown using information from previously
grown trees. Boosting does not involve bootstrap sampling; instead each
tree is fit on a modified version of the original data set

Boosting has three tuning parameters:

1. The number of trees B . Unlike bagging and random forests, boosting
can overfit if B is too large, although this overfitting tends to occur
slowly if at all. We use cross-validation to select B .

2. The shrinkage parameter λ , a small positive number. This controls the
rate at which boosting learns. Typical values are 0 .  01 or 0 .  001, and
the right choice can depend on the problem. Very small λ
can require using a very large value of B in order to achieve good performance.

3. The number d of splits in each tree, which controls the complexity
of the boosted ensemble. Often d = 1 works well, in which case each
tree is a stump , consisting of a single split. In this case, the boosted
stump ensemble is fitting an additive model, since each term involves only a
single variable. More generally d is the interaction depth ,and controls
interaction depth the interaction order of the boosted model, since
d splits can involve at most d variables.

boosting cannot easily be paralazable

2 types of stochastic boosting

adaboost is another boosting
gradient boosting outperforms adaboosting

a simple (boosted) tree, high bias, low variance
many (boosted) trees, low bias, high variance

ensemble is a type of algorithm
********************************

REPO: Neural Nets

********************************
When to use Neural Nets:

1) Don't use if you have a linear model
2) Large amount of data over 500,000 observations
3) Can approximate arbitary functions

Grid search to see what features would work well
Supervised 

self organized map is unsupervised neural net

Large data is 500,000 to over a million 

3 Types of Neural Nets:

1) Feed forward . Info is fed unidirectionaly through the net
2) Recurrent - nerual net with directional update and feedback loops
3) Unsupervised learning - self organizing maps

Feed Forward 

1) Input layer
a) # nodes = # features
b) input are our observations

Produces :

V = a0 + a1*x1 + a2*x2 + ... + an*xn

2) Hidden layer (without hidden layer then just regression)
a) user defined activation funcion such as sigmoid + 1 + hidden layers
b) inputs are a weighted sum of the previous layer

activation functions:
1) sigmoid family : f(x) = 1 / ( 1 + exp(-x)) 
bound by 0 and 1  - used as building blocks for abritary function building
2) tanh : f(x) = 
bound by -1 and 1
2) relu : f(x) = 
linear response unbounded

3) Output Layer

a) # nodes is output dependent

i) regression setting = 1 note - which is the final quantitative result
ii)  classification setting = # classes - probability for each class

b) inputs are a weighted sum of the previous layer

c) Activation function

4) Bias terms - analogous to the intercept in a linear regression as an input without a node

Feed Forward with one hidden layer:

"Fully connected network" or "Dense network" shown with nodes.

Weights are randomly initialized. Need to have normalized features so that all can be compared
to each other.

Step function could be approximated by random forest

U to z transition if linear is the same value 

x -> v -> y -> U ->  compare to t

Another example:

x = 0.5 

aij are random weights

v1 = a01*1 + a11*x = 20.8 * 1 + 0.69 * 0.5 = -13.7
v2 = a02*1 + a12*x = 47.6 * 1 + 0.68 * 0.5 =  13.6

y1 = 1/ (1 + exp((13.7) ) = 1.1122 * 10 -6  
y2 = 1/ (1 + exp((-13.6) ) = 0.999

U = b0 * 1 + db1 * y1 + b2*y2 
  = 0.25 - (0.5) (1.22X 10 -6) + (0.5) (0.9999)
 = 0.75

U = z = 0.75

apply sigmoid function to get output

Backward Neural Nets
Training: Adjust weights using error metric through 
back propogation

1) MSE = (0.5 - 0.25) ** 2 = 0.0625

Start with error term and work backwards throught these weights:

Take the partial derivative (pd) of error with respect to b1

which is equal to Z -> final output using the chain rule

GRADIENT:

pd(E) / pd(b1) =(  pd(E)/ pd(Z) ) * ( pd(Z)/ pd(U) ) * ( pd(U) / pd(p1) )

Problem with multiplying the derivative may cause an exploding or 
a vanishing network for the gradient

Multilayer Perceptron (MLP) is a 
is a feedforward artificial neural network model that maps sets of input 
data onto a set of appropriate outputs. 

model = sequential()
model.compile(loss = 'categorical_crossentropy',optimizer='RMSprop')
model.add(Dropout(0.35))

#
# dropout randomly choose observations that the hidden layer sees
# so the nodes become more independent with less data.
# same as regularization and max step on trees
#
#

Regularization or dropout will decrease variance in the model 
Prevents overfitting. 

Up complexity is modeling noise. 

Dropout allows for each node to have less say and reduces complexity

keras library

*******************************

********************************

REPO: SVM

********************************

hyperplane split

C tuning parameter or budget:

C small is low bias, high variance
C large is high bias, low variance
Increase C increase bias and lowers variance more support vectors
As C increases more error is allowed. 

gamma is another training parameter or 1/r or radius of influence of the support vector.

r is the radius of influence.

1) skilearn gamma search over cross validation
2) if data has separability problems in logistic regression svm should be better

Very slow.  Does not scale very well.

Should consider logistic regression, k nearest neighbors, random forest, 
boosted tree

Benefit is to recast data with a kernel such as a guasian to transform data
into another dimension where they are separatable in another dimension

Need to normalize

b + w T * x = 0 

maximize distance between groups - is reducing variance and increasing bias

inducing bias is called - regularization

margin is called support vectors with points in margin. Outside of margin is explicity
classified. Each of those points is called a support vector. Need these vectors to define
points between the classifications

Measure epsilon from margin to points which is a tuning parameter which is summed
into C which is called "budget"

More vectors are decreasing variance or regularization

Soft margin.

Can model a circle of data in a donut by transforming the data:

b0 + b1x1 + b2x2 +b3x1**2 + b3*x2**2 + b5x1x1

into a higher space.

Accomplishes this with a dot product - only values with positive alpha values are scored the
rest of the points in the margin are set to zero so that they don't have an influence on the
hyperplane.

Different kernels determine measurement of similuarity
The Gausian kernal or RBF radial basis formula
=K(x, xi) = exp(-gamma * abs( x - x2)**2) 

Budget for bias/variance tradeoff ? C for making errors

curse of dimensionality  ?? 

as explained by : http://stats.stackexchange.com/questions/83686/the-curse-of-dimensionality-linear-svms

Instead of the complexity of the model being defined in terms of the number of model parameters (which is a traditional view in statistics), the parameter C is used to define a nested set of classes of models of increasing complexity. The task is then to produce a model that explains the data from the model class of the least complexity. This is known as structural risk minimisation.

In other words the linear SVM is robust to the curse of dimensionality provided the C parameter is tuned very carefully. As @Marc Claesen suggests (+1), if the model still doesn't give good performance, it is probably a non-linear problem and an RBF kernel is a good option. 

The support vector machine (SVM) is a generalization of a simple and 
intuitive classifier called the maximal margin classifier.

Hyperplane - In a p-dimensional space, a hyperplane
is a flat affine subspace of hyperplane dimension p − 1.

A natural choice is the maximal margin hyperplane
(also known as the maximal margin hyperplane optimal separating hyperplane
), which is the separating hyperplane that optimal separating hyperplane
is farthest from the training observations

Advantages:

1) Greater robustness to individual observations
2) Better classification of most of the training observations.

The support vector machine (SVM) is an extension of the support vector
support vector machine classifier that results from enlarging the 
feature space in a specific way, using kernels .

********************************

REPO: Boosting

********************************

Model process:

make model f0(x)
for m in M:
    take the errors from f_m-1(x)
    fit a model to those errors with a decision tree T_m
    update total model:

        f_m(x)  <-- f_m-1(x) + T_m(x)

Boosting is not Bagging or random forest:

Bagging - bootstrap many trees, each tree independently grown,
in an effort to decrease variance through averaging

Random Forest - Similiar idea, but take random subset of possible 
freatures at each split to "decorrelate the trees"

Gradient boosted regression trees:

- instead of fitting to reweight trainig observations, fit residuals 
to of previous tree

increased complexity increases variance to fitting the next model
bias is the residual sum of squares


greedy model is GBRT with shrinkage factor 

AdaBoost - Each tree is expert on attacking errors of predecessor
iteratively re-weighs observations based on errors

Discrete AdaBoost

Use trees with a depth 4 - 6 

Gradient boosting stump to 8
Random forest is deepest posible

GBRT is sklearn - tuning - 
1) set n_estimators high as possible
2) tune hyperparameters together via grid search

from sklearn.grid_search import GridSearchCV

pram_grid = {'learning_rate':
'max_depth':
'min_sample_leaf':
max_features': }

est = GradientBoostingRegressor(n_estimators=3000)
gs_cv = GridSearchCV(est, param_grid).fit(X, y)
gs_cv.best_params_


***************

Bagging/RF/Boosting
Feature and Variable importance

1) Feature 
2) Partial Dependency plot - gives relationship with outcome - fix
variable value and see outcome when it goes through the model
look at the average predicted value
look for the shape of the relationship with the data
how much the dependent variable changes

3) Stochastic gradient boosting
4) correlated predictors have multicolinearity so can handle corelation but linear regression cannot
Not harmful to these models
random forests accounts for this because of the subset feature selection 
gradient boosting has the all features so handles correlation differeently


start with random forest and go to gradient boosting

training data in a random forest :

random selection with replacement
bagging - boostrap aggregation - oob - out of bag error rate
use a subset of features (sqrt(n)) recommended

1) Calculate oob error rate - train tree model - and calculate oob
2) Shuffle values X1 or permute 
3) Permute - and recalulate error rate

********************************

REPO: Assessment-4
      Profit-Curves

********************************

REVIEW

false negatives are type II errors
false positives are type I errors

two methods of predicting in python

.predict_proba()
.predict()

######################################################################

WEEK 5 Natural Language Processing

######################################################################

REPO: web-scraping

********************************

Natural Language Processing

web_scraping_lec.pdf

mongodb

document-based database (json)  - 

document-oriented databases are inherently a subclass of the 
key-value store

A document-oriented database or document store is a computer program
designed for storing, retrieving, and managing document-oriented 
information, also known as semi-structured data. 

semi-scalable
good for storing unstructured data
schema-less
no joins
suboptimal for complicated queries
no transactions

lecture.md

mongodb - no explicit concept of joins
included with mongodb

default port is 27017

has databases
databases have collections
collections are unstructutured

need to start server if running "mongo" command errors

Commands:

show dbs

create database by:

use new_databasename

db.users.insert({ name: 'Jon', age: '45', friends: [ 'Henry', 'Ashley']})

creates "users" collection by inserting values

key value pairs like dictionaries

camel case because java based

command list:

*****************
use new_databasename
use nancy
use local
db.users.insert({ name: 'Jon', age: '45', friends: [ 'Henry', 'Ashley']})
db.getCollectionNames()
db.users.find()

// find by single field
db.users.find({ name: 'Jon'})

// find by presence of field
db.users.find({ car: { $exists : true } })

// find by value in array
db.users.find({ friends: 'Henry' })

// field selection (only return name)
db.users.find({}, { name: true })

// update

db.users.update({name: "Jon"}, {friends: ["Phil"]})

// replaces friends array
db.users.update({name: "Jon"}, { $set: {friends: ["Phil"]}})

// adds to friends array
db.users.update({name: "Jon"}, { $push: {friends: "Susie"}})

// upsert
db.users.update({name: "Stevie"}, { $push: {friends: "Nicks"}}, true)

// multiple updates
db.users.update({}, { $set: { activated : false } }, false, true)

*****************

mongo has upsert update or insert
MongoDB is not ACID compliant. 

When to use sql and when to use mongodb

sql

structured - optimized for query speed
reliability - guarentee uniqueness

nosql

less reliability - no index integrity
unstructured data - can handle missing columns
web scraping
easy to import from jason


from pymongo import MongoClient
from bs4 import BeautifulSoup
from requests import get
from datetime import timedelta, date

def test_pymongo():
    client = MongoClient()
 
    #Initiate Databse
    db = client['test_database']

    #initiate table
    tab = db['test_tabel']

    test_insert ={'author':'John Doe', 'title': 'HilBilly Jane'}

    tab.insert_one(test_insert)

    print 'Original Result'
    test_result = tab.find({'author':'John Doe'}).next()


    print test_result

    print "Updated Result"
    tab.update_one({'_id': test_result['_id'] }, {'$set': {'author': 'John Doe2'}})

    print tab.find({'_id':test_result['_id']}).next()

def read_ebay_shoes():


get get data without any handshake with website
post send along information to get data from website

<script>inline java script </script>
<script>tag=link to java file </script>

<css>

styles pages to look pretty
css selector for h2 headings
can pick any attribute in the html page
ids are unique to the page
classes can be applied to multiple elements
class can be applied to multiple elements referenced by the css file

cascading style sheets
children get same styles

ipython

from requests import get

html = get('http://www.ebay.com/sch/i')

html.

html.status_code 

200 is good

html.content

from bs4 import BeautifulSoup
bs = BeautifulSoup(html, 'html.parser')

TypeError: object of type 'Response' has no len()

bs = BeautifulSoup(html.content, 'html.parser')

bs.select

use chrome "inspect element"

to see html code to look for class of interest

creates a list:

vip_tabs = bs.select(".vip")
first_tag = vip_tabs[0]
type(first_tag)
type(bs)

first_tag.get('a')

first_tag.text

parsing review:

loop 

for url in urls:
    html = get(url)
    parsed_html = BeautifulSoup(html.content, 'html.parser')
    vip_tags = parsed_html.select('.vip')
    for tag in vip_tags:
        test = tag.txt
        with open('my_tags.txt') as f:
            f.write(text)

file io is expensive 
better to keep a list of text tags 
and write all at once

should minimize the number of times to open and close a 
file

best practice to only access a url once

********************************


REPO: npl

********************************

corpus: collection of docs => "data set"
document: single entity in corpus, a string
vocabulary: set of words appearing in corpus
bags of words: vectorized document, word counts
stop words: to ignore (articles, conjunctions) => "non-informative"
token: single word => "tokenize" list

Tokenize documents: list of unigrams
Binary occurrences/ counts:
Frequencies
TF-IDF

Better than frequencies: TF-IDF
Term Frequency
Inverse Document Frequency
Non informative words such as "the" are scaled down

skikits.learn

from skikits.learn.features.text import WordNGramAnalyzer
text = (u"J'ai mange\xe9 du kangourou ce midi,")

WordNGramAnalyzer(min_n=1, max_n=2).analyze(text)
from skikits.learn.features.text import CharNGramAnalyzer

analyzer = CharNGramAnalyzer(min_n=3, max_n=6)
char_ngrams = analyzer.analyze(text)

print char_ngrams[:5] + char_ngrams[-5:]

from skikits.learn.features.text.sparse import Vectorizer
from skikits.learn.sparse.svm.sparse import LinearSVC

vec = Vectorizer(analyzer=analyzer)
features = vec.fit_transform(list_of_documents)
clf = LinearSVC(C=100).fit(features, labels)

clf2 = pickle.loads(pickls.dumps(clf))
predicted_labels = clf.predict(features_of_new_docs)

import nltk
nltk.download() - gui window pops up

pos_ids = reviews.fileids('pos')
neg_ids = reviews.fileids('neg')
len(pos_ids), len(neg_ids)
reviews.words(pos_ids[0])

The pickle module implements a fundamental, but powerful algorithm for serializing 
and de-serializing a Python object structure. “Pickling” is the process whereby a
Python object hierarchy is converted into a byte stream, and “unpickling” is 
the inverse operation, whereby a byte stream is converted back into an object 
hierarchy. Pickling (and unpickling) is alternatively known as “serialization”, 
“marshalling,” [1] or “flattening”, however, to avoid confusion, the terms used 
here are “pickling” and “unpickling”.

EXAMPLE:

3 documents in corpus:

D1: "Baseball is clearly superior to football !!"
D2: "Lake Superior is the largest Great Lake."
D3: "Large avacados are a great source of vitamins."

goal is to move from corpus to useful analysis

how to begin?

1) remove punctuation
2) remove stop words
3) lower case
4) stemming /Lemmatization of the words (e.g. largest becomes largest)
Porter/Snowball
Wordnet Lemmatizer

5) parse strings into words or tokenize

now move to a dataset

each word is a feature and each document is a row

bag of words model:

count vectorizer:

doc baseball football superior lake great clear large ...
1      1         1     1         0   0     1     0
2      0         0     1         2   1     0     1
3      0         0     0         0   1     0     1

as documents increase the matrix becomes wider and
sparser

number of features (p) is larger than (n) documents
no regression because more p > n so no finite solutions available

this is called "term document matrix" 


problems with term frequencies:

1) row vectors of counts not normalized (e.g. use cosine similiarity or normalize by 
document length)

2) uncommon words across docs
idf(term, Document) = log(Numberofdocs/countofnumberofdocumentswhere_term_exits)

or the length of the set that the term appears
or the sum of the times it appears in the Corpus

what we want:

TF-IDF - term frequency inverse document frequency

Three options:

1) raw counts (tf) term frequency
2) log(1 + tf) # add 1 to avoid taking the log of zero
3) 0.5 + (0.5 * tf /max(tf) )

or maximum frequency normalization in general alpha terms:

3) alpha + ( (1 - alpha) * tf /max(tf) )

4) TfidfVectorizor = L2norms

back to EXAMPLE:

"baseball" is in {1}
"superior" is in {1, 2}
"great" is in {2, 3}

great = log (3/2)

N = number of documents or observations
D = corpus which contains all documents d
term frequency returns term information for the document

tf(t,d) * idf(t, D)

tf(t,d) can be normalized different ways.

compare matrixes with one row (document) to another:

use cosine similiarity
a dot b / cos(angle)

naive bayes for text classification:

Bayes Rule:

P(a|b) = P(b|a)*P(a) / P(b)

Likelihood: P(b|a) is prob b given prior b
P(a) is prior
P(b) is prob b occuring across all possible a

Simplifications:

P(a|b) is prob of label given text

1) P(a|b) is proportional to P(b|a) * P(a)
2) bag of words - order of words does not matter
3) indepence of features (words) - for naive bayes to work

Naive Bayes:

P(xi | cixj ) = P(xi | c)

For New York Times - P Netanyahu given that Int'l and Isreal exists

is the same as 

P(Netanyahu| Int'l)  P netanyahu given Int'l exists

find the "the cat in the hat" probability given the label C

P("the cat in the hat" | C) * P(C)

which is:

P(C) * P("the"|C)**2 * P("cat"|C) * P("in"| C) * P("hat"|C)

multinomial naive bayes classifier
naivebayes.pdf

calculate for every class and then choose the class with the highest
probablity which becomes argmax P(C)

c = argmax P(cj) * pi P(x|c)

multipling together prob leads to very small numbers

instead take log so values are still meaningful:

= log(P(c)) + sum (x_w * log P(w | c))

P(C) = sum ( y_i == C)/ N

number of lables equal to c divided by all documents

P(w_j | c) = count(w_j, c) / (sum ( count(w, c))

count of word of interest / total words

c is a subset of the data

laplace smoothing:

Likelihood the word is in each class:

adds a 1 to the prob for positive probality space see slide 35 in naivebayes.pdf

P(w_j | c) = ( 1 + count(w_j, c)) /( (sum ( count(w, c)) + |vocab| )

easy to train so good place to start
disadvantage of a strong assumptions
better job with ensemble classifier
easy to test in sklearn like all the other

p can be greater than N so no worries of overfitting

********************************

REPO: clustering

********************************
lecture.pdf

1) K Means 
2) Hierarchical

Unsupervised learning.

Algorithm:

1) Initialize - Randomly assign to a cluster

2) Until convergence or assignments are no longer changing 
or that your number of iterations runs out 
or tolerance is met:

    2.1) while for each k evaluate centroid
    2.2) reasign obs to closest centroid using
         euclidean distance

Could end up with suboptimal points because of random 
assignment. Fix with doing step 1 e.g. 10 times.

Get best solutions. Choose set that minimize the sum of
squares.

sklearn.k-means++ : implements this as it is better than random 
assignment. this helps converge faster. 

n_jobs = -1 has jobs on all available processors

How to choose best number of k:

1) Evaluate within cluster sum of squares (WCSS)
Graph points and pick lowest point with the minimum number of points.
called elbow method. plot total sum of squares for all clusters
versus number of clusters

2) Silhouette Coefficinet (b-a) / max(a,b)
negative values are bad.
looking for highest value closest to one

http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_ana

Eucledean distances:

1) must normalize data
2) be aware of the curse of dimensionality

parallelization:

each iteration of random assignment is not dependent on the other. 
makes each iteration easily parallelizable

hierarchical clustering
agglomerative

looking for smallest value from pearson's correlation:

1 - corr(xi, xj)

*******************************

unsupervised hierachial clustering

how to link clusters with heirachy:

1) complete linkage:

next cluster the farthest from any two observations already
existing in the cluster to join two heirarchies

2) single - opposite of complete linkage is single :

closest distance is single distance to join two heirarchies

3) average 

4) centroid distance is minimized

most used are complete and average

********************************

REPO: case-study

********************************


********************************

REPO: dimensionality-reduction

********************************



######################################################################

WEEK 6 Unsupervised Learning

######################################################################


********************************

REPO: Assessment-5
      nmf

********************************


********************************

REPO: image-featurization

********************************


********************************

REPO: recommendation-systems

********************************


********************************

REPO: recommender-case-study

********************************


********************************

REPO: time-series

********************************

