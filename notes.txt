LOGISTICS:

EDU password: schoolhouserock
2.4: ourmembersrock!

STUDY:

pandas query
list comps for dataframes
enumeration
library itertools
iteration
key value pairs for dicionaries

lambda with map and reduce

######################################################################

WEEK 1 Software Engineering + EDA

######################################################################

REPO- Assessment-day1
      pthon-intro

********************************

constructor -> creates object --> __init__ magic method
method -> function which akkcts on an object

CONCEPTS:

argmax

argument with the max value

MAGIC METHODS:

overload __name__

less than, greater than, equal __cmp__ cmp() method
allows for == < > >= <= overloading

__repr__ for returning string for printing

__init__ constructor

IPYTHON:

_ -> last resutl
_i10 -> result of line number 10
im <tab> last command that started with "im"

LISTS:

pop
extend
append

if not list_name then it is empty

ENUMERATION:

range generate all values while xrange does one at a time to save memory

for i, item in enumerate(L):
    print i, item

which is faster than a for loop

enumerates zip with izip which combines lists item by item
count is a sequence

from itertools import izip, count

for i, first, last in izip(count(), first_names, last_names):
    print i, first, last

LIST COMPREHENSION:

doubled = [item * 2 for item in L]

L = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

doubled = [[item * 2 for item in row] for row in L]

Flatten 2-D list:

L = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

[item for row in L for item in row]

[1, 2, 3, 4, 5, 6, 7, 8, 9]

********************************

REPO: sql

********************************

POSTGRES:

sudo su postgres
user postgres
psql dbname
\d # tablenames
\d tablename # column names
\q # quit

select userid, campaign_id, events.meal_id, type, 
price, event from events inner join users USING(userid) 
inner join meals ON meals.meal_id = events.meal
where event = 'bought';

select  count(events.meal_id), type

from events inner join users USING(userid) inner join meals 
ON meals.meal_id = events.meal_id where event = 'bought'
GROUP BY type;

CREATE ROLE username superuser;
CREATE ROLE username postgresql;

WITH user_campaign_counts AS (
    SELECT events.userid, count(*) as bought, campaign_id
    FROM events
    JOIN users
    ON users.userid = events.userid
    AND events.event = 'bought'
    GROUP BY events.userid, campaign_id ),

max_campaign_bought AS (
    SELECT campaign_id, max(bought) AS bought
    FROM user_campaign_counts
    GROUP BY campaign_id)

SELECT u.userid, u.campaign_id
FROM user_campaign_counts u
JOIN max_campaign_bought m
ON u.campaign_id = m.campaign_id AND
   u.bought = m.bought

Find all meals which are above average price
of previous day.

self join example:

SELECT current.dt, current.meal_id
FROM meals current
JOIN meals previous
ON previous.dt >= current.dt - 7 AND
   previous.dt <  current.dt 
GROUP BY current.dt, current.meal_id, current.price
HAVING current.price > AVG(previous.price) 
ORDER by current.dt;

******************************************************
For each user count # share events and # like events.
******************************************************

SELECT userid, 
       SUM(CASE WHEN event = 'share' THEN 1 ELSE 0 END) as share, 
       SUM(CASE WHEN event = 'like'  THEN 1 ELSE 0 END) as like 
FROM events
GROUP BY userid
ORDER BY userid;

SELECT userid, COUNT(*)
FROM events
WHERE event = 'share'
GROUP BY userid;

SELECT userid, COUNT(*)
FROM events
WHERE event = 'share'
GROUP BY userid;

SQL INTERFACE WITH PYTHON:

1- Open a connection
2- Initialize a cursor object
3- Execute a query
4- Commiting the results
5- Close the connection

import psycopg2
from datetime import datetime
conn = psycopg2.connect(dbname= 'socialmedia', host='/tmp')
c = conn.cursor()

c.execute(
    '''
    CREATE TABLE logins_7d_%ss AS 
    SELECT userid, COUNT(*) AS cnt
    FROM logins
    '''
    )


c.close()
conn.commit()
conn.close()

conn.rollback() 

c.execute(
    '''SELECT userid1
       FROM friends;'''
)

Cursor object is a generator. Each row is loaded one at a time.

l = list(c.fetchall())

c.next()

a = []

for result in c:
    a.append(result)

c.fetchall() -- returns list of tuples

********************************

REP: slq-python
     PANDAS

********************************

from math import sqrt
import random

import numpy as np
import pandas as pd
import scipy.stats as scs

PLOTTING

import matplotlib.pyplot as plt
import seaborn as sns

# ipython notebook

%matplotlib inline

pd.plot ANYTHING
sns.whatever

# ipython command line

plt.show()

# numpy data array load from csv

google = np.loadtxt('data/lunch_hour.txt')

# pandas data frame load from csv

df = pd.read_csv('data/salary_data.csv')
df.describe()

Pandas in built on numpy which is fast

ipython notebook

index 3 ways:

ix - more flexible index

loc - non-numeric index
iloc - integer index locations

boolean indexing is masking

(s > 0).head()
s[s > 0].head()

iloc[:,1:]

rows, columns

Quantitative var

histogram (order is important and bin is chosen)
kernel density estimates
normal quantile plot - for normal distribution check
Stem - leaf plot

Qualitative var

Bar chart (order is not important and bin is inherient)
Pie chart
Pareto chart is bar chart that is ordered by counts

Bivariate:

Qual & Qual - Scatter plot 
Quant & Qual  -  Box and whisker within each category side by side

Qual vs Qual: 

mosaic plot
multiple pie
stacked bar


######################################################################

WEEK 2: Statistics and Probability

######################################################################

REPO: Assessment 2
      Probability

********************************

Don't care on what you will believe if something
will happen. "confidence"

Probabity is on history

Two ways to think of frequentist probability:

1) (# of events that meet your criteria) / (total # of possible events)

2) How often does this happen in the long run ?

***********************

BAYESIAN PROBABILITY:
another way to look at probability is by Bayes Rule and 
Bayesian Probability:

Fundamental belief - not strictly data - that something is true not changed
by history. "chance","likely hood" , "previous" or prior, 

"Prior Belief" is essential to Bayes 

prior - probability distribution of one's beliefs before collecting any data
        equal weight to all called "uninformed prior"
        Posterior becomes prior for next round 

likelihood - Probability of data given some belief / outcome 

Posterior - probability distribution of one's belief after collecting data
            Multiply prior * likelihood of given data value

Conjugate prior - if the posterior and prior are from the same family of 
probability distribution, the are conjugate distributions, and the 
prior is a conjugate prior to the likelihood

Define two events A and B

Bayes Rule:

P(A | B ) = P(A) * (P( B | A)) / P(B)

Mutually exclusive vs. Independent events
(rain, no rain - cannot occur at the same time)

Independent Events: P(A and B) = P(A)*P(B)

Rewriting using Law of Total Probability:

P(A- | B ) = P(B|A)* P(A) /( P(B|A) * P(A) + P(B| A') * P(A'))

2)

*******************************************

one set of events is called marginal probabilies
multiple sets of events is called joint probabilies

*******************************************

Random variable is a capital letter.
specific value is a lower letter

~ is notation for distributed as:

X ~ Normal(mean, variance ** 2)

*******************************************

Four parts important for distributions:

1) Measure of center (median, mean, mode)
2) Measure of spread (5# summary (min, max, q1, q2, q3) , variance, range, std. dev)
3) Shape (skewed, symmetric)
4) Outliers

*******************************************

CDF -> Cummulative Distribution Function
PMF -> Discrete or qualitative probability mass function
PDF -> Continuous of quantitative probability density function (CDF derivitive)

*******************************************

Expectation or Expected value is the weighted average

Sum up each value * probability of each value

*******************************************

Variance - measure of the variablity around x

Var(x) = Expected_value(x ** 2) - (Expected_value(x)) ** 2
       = Expected_value(( x - Expected_value(x)) **2 )

Expected_value(x**2) = sum_of ( x**2 * p(x) )

Standard Deviation is the square root of variance in order to compare units

Discrete random variables

*******************************************
HOW TO MODEL
*******************************************

1. Identify your random variable (R.V. X or Y e.g. height)
   (quantitative, qualitative, start min, end max values, continuous, discrete)
2. Identify the distribution
3. Identify the parameters
4. Answer the question of interest

*******************************************
DISTRIBUTIONS
*******************************************

Bernoulli ( p ):  one if a coin with heads probability
        p comes up heads, zero otherwise. (one coin flip)

Binomial ( n, m, p) : the number of heads in n independent flips of a coin with heads probability p .
Only takes on two events. Sum of Bernoulli. (multiple coin flips)

(n x) = n! / (x! ( n-x) !)

Expected value = n * P

Geometric (p) (where p > 0): the number of flips of a coin with heads probability 
                             p until the first heads
            binomial distribution with number of success until failure
            e.g. 95% free throw shooting percentage

X = makes of shots - binomial
X = makes of shots until first miss- geometric
X = make 8 shots - with a 95% rate - binomial
scs.binom.pmf(8, 10, .95)

Negative Binomial - geometric combined until n misses e.g. trial until n (e.g.) 3 misses

Poisson (lambda) (where lambda > 0): a probability distribution over the nonnegative integers used for modeling the frequency of rare events
Number of events that can happen in an interval.
Positive discrete. Right skew. Non-symetric. 

lambda = mean = variance

Continuous random variables:

Uniform (a,b) (where a < b):  equal probability density to every value between a and b on the real line.

******************

Beta ( alpha, beta ) continuous between 0 and 1 - generally for proportions

Beta(alpha, beta)  = c ** ( alpha - 1) * ( 1 - c ) ** (beta - 1) / ( beta(alpha, beta) )

c = click through rate / conversion rate

Beta ( 1, 1) is not any information
Beta ( 500, 500) this gives enough samples for konwing what is going on

Gamma( alpha, beta ) 
expected value = alpha * beta
variance = alpha * beta ** 2
Right skewed continuous distributions .  Range 0 to positive infinity.

special cases of Gamma:

******************

Chi Squared ( degrees of freedom )  : independece test of two 
categorical variables  beta = 2 alpha = df/2
or ... Gamma( df /2 , 2 )
also used for comparing proportions

df = (nrows - 1) * (ncols - 1)

Used to compare two categorical groups for independence

Test your variance equal to some value

p value is always shaded to the right

H0 Independent
H1 Not Independent of two categories

******************

standard normal 0, 1

Exponential (lambda) : decaying probability density over the nonnegative reals

******************

F-distribution (dfn, dfd)
t-distribuion (df)

******************

Normal (mean, standard deviation ** 2) :  also known as the Gaussian distribution 
variance = standard deviation **2
Model over entire real line
min = negative infinity
max = positive infinity
expected value is the mean
--> good for averages and porptions

Gaussian random variables are extremely useful in machine learning and statis-
tics for two main reasons.  First, they are extremely common when modeling 
“noise” in statistical algorithms. Quite often, noise can be considered 
to be the accumulation of a large number of small independent random 
perturbations affecting the measurement process; by the Central Limit 
Theorem, summations of independent random variables will tend to 
“look Gaussian.” Second, Gaussian random variables are convenient for 
many analytical manipulations, because many of the integrals involving 
Gaussian distributions that arise in practice have simple closed form 
solutions.  We will encounter this later in the course

For any continuous distribution no probability for a single point.

******************
SCIPY
******************

import scipy.stats as scs 

# for binomial distribution

scs.binom.pmf(x, n, p)

# X = make 8 shots out of 10 - with a 95% rate - binomial

scs.binom.pmf(8, 10, .95)

# X = make at least 8 shots out of 10 - with a 95% rate - binomial
# take results minus 1 to get the part of the chart

1 - scs.binom.cdf(7, 10, .95)

******************

4 things to get back:

scs.nameofdistribution. and then...

1 - pmf - P(X = x) discrete qualitative
          probability mass function value at instance 

* - pdf - P(X = x) continuous - for plotting not solutions - so not counted for use
          probability density function (quantitative continuous) should be 0

2 - cdf - P(X <= x) may need to subtract from 1
          Cummulative Distribution Function under a range P(X <= x)
3 - ppf - returns x given probability (quantile) "backward calculation"
          P( X <= x) for discrete and continuous e.g. how many shots in 85% of the time?
4 - rvs - size for random draws for the distribution

******************

Estimation Techniques for Distribution Parameters

******************

1 - Maximum Likelihood Estimations (MLES)
2 - Method of Moments (MOMs)
~ 3 - Maximum a Posterior (MAPs)

MLES is Calculus Based:

Asymptopic statistical properties are with large sample sizes

Calculate:

1) compute joint distribution (likelihood)
2) log the joint distribution
3) take the derivative with respect to your parameters
4) set to 0
5) solve for parameters

Example Poisson distribution:

1) f(x|lambda) = lambda ** x e ** -x / x!

2) likelihood ( lambda | x_sub_i ) which is proportional to the log 
sum ( x_sub_i * log(lambda) - lambda * n )

5) lambda = mean

******************

Easiest is MOMs which is Algebra Based:

1) take expected value and equate it to a moment

First and second moments for all distributions:

E[X] = 1/n sum x_sub_i = M1
E[X**2] = 1/n sum x_sub_i ** 2 = M2

variants for all distributions:

VAR[X] = E[X**2] - (E[x]) ** 2

e.g. for a normal distribution:

VAR[X] =  sigma ** 2

e.g. for a beta distribution:

X ~ Beta ( alpha , beta)

Expected value:

E[X] = alpha / (alpha + beta)

VAR[X] = alpha * beta / ( alpha + beta ) **2 (alpha + beta + 1 ) )

Calculate M1 and M2 from values using moment calculations:

M1 = alpha / (alpha + beta)
M2 - M1 ** 2 = alpha * beta / ( alpha + beta ) **2 (alpha + beta + 1 ) )

********************************

REPO: Sampling and Estimation

********************************

Populations/Parameter/Sample/Statistic

******************

Statistic is numeric from sample (lowercase symbol)
Parameter is numeric from population (Greek symbol)

******************

Central Limit Theorem

******************

the statistics distribution are normally distrution will
be normal with a large enough sample size. 30 is a good 
guess

******************

Sampling Distributions

******************

Distribution of a statistic. 
X ~ N(mean, (variation/sqrt(n))**2)

Example:

Consider I am interested in every
person at Galvanize's average
commute.

I select 50 people at
random from the 
building.

I find an average commute of 35 min
with a standard deviation of 20 minutes.

DO FIRST:

Population: Everyone at Galvanize
Parameter: The average time to commute (mue)
Sample: 50 people selected 
Statistic: x = 35 min s = 20 min

x is a normal distributed from the central
limit theorem

How to determine the average travel time for 
everyone? 35 +- 2 deviations

******************

CONFIDENCE INTERVALS

******************

Only good for sample statistics

t* - is a t* distribution for a sample
by degrees of freedom

x +- t* sub (n - 1) * (s / sqrt(n))

Not really used but only if you large sample
size

Z* - is a Z* population distribution

x +- Z* (s / sqrt(n))

95% confidence of data in the middle of the curve.

35 +- scs.t.ppf(.975, 49) * 20 / sqrt( 50 )

scs.t.pdf(.975, 49)

1 - 0.975  = 0.025 

+- ----> give 5%

area on the left 
50 sample size - 1

We are 95% confident the average commute for all 
Galvenize travelers is between ( and ).

*use confident instead of chance

******************

Bootstrapping

******************

Repeated samples from samples space.

Treat sample as a population.


********************************

REPO: Frequentist Hypothesis Testing

********************************

all assume normal distribution

Null hypothesis H0 is what you assume to be true before collect any data.

1. H0 alternative hypothesis H1
2. Figure out what test. Calculate some test statistic.
common z (proportions), t (means), chi squared, F tests, 
Welches) from a distribution
3. Obtain p-value
4. p-value <= alpha then reject null hypothesis
(determines whether or not alternative H1 is accepted)
5. Conclusion in terms of your problem.

p-value is greater than alpha when you reject the null hypothesis

Power = 1 - beta = of the test is the probability to choose H1 correctly

Parachute Example:

type 1 error: Worst type of error - alpha  - think the parachute 
will open but it does not. Chose H1 when H0 is really true

scs.norm.ppf(0.95, 0, 1) z score

type 2 error: beta - think the parachute will not open but it does
Chose H0 when H1 is really true

cdf to get area under curve

how to calculate the value of beta...

effect size is the difference between two means

Hypothesis Testing Example

Consider I believe the average commute time to
galavanize is 30 minutes or less for all 
individuals

Null hypothesis H0 is what you assume to be true before collect any data.
Alternative hypothesis H1 is what you want to test that you think is true. 

n = 50
mean = 35
std = 20

H0: m >= 30
H1: m < 30

use t statistic for a single test

t = (est - value H0/ SE est)

df = n-1

H0 t = (35 - 30)/ 20/sqrt(50) = 1.77

scs.t.ppf(0.975, 49) 

3 - 1.77 value p value shaded to the left of 1.77 because of less than

if scs.t.cdf(1.77, 49)  > .05 then failed to reject null hypothesis

conversely if null hypothesis is < 30 then :

if 1 - scs.t.cdf(1.77, 49)  < .05 then reject the null hypothesis

******************

for two means use paired t test 

df = n-2 for 2 means

for two means use unpaired t test 
e.g. Welches test

(x1 - x2)  - (mean1 - mean2) / (SE (x - x2)

df = n-2 for 2 means

for test proportion normal z or 0 - 1

for compare two proportions normal z or 0 - 1

one sample proportion 

df = n-2 for 2 means

proportion has z test
means has t test

******************

Publishing bias 

multiple comparison techniques

5 publish can have type 1 errors
while 95 cannot publish that do not have type 1 errors

Bonferroni Correction: new alpha = old_alph / number_tests

******************

p-value

The probability of getting your data or more extreme data
if the null is true

P(x | H0 is true) 

******************

When you have really large sample sizes everything is statistically
significant

Need to use something other than a p value.

Confidence interval can also be used as a hypothesis test.

Know what the result of your test is "saying"

e.g.

H0: m1 = m2    
H0: m1 - m2 = 0

H1: m1 != m2

Shade the alternative hypothesis

******************

Experiments vs observational studies

Experiments assign individuals to a treatment


********************************

REPO: Power Calculation and Bayes

********************************

Bayes Rules

https://en.wikipedia.org/wiki/Conjugate_prior

Conjugate priors are nice! Beginning belief of the distribution.
Posterior has the same distribution

1. Simple Example

Data follows a Poisson distribution

Conjugate prior is a gamma distribution because is goes from 0 to infinity
with the same range as the distribution of the MEAN. Feeding in 
distribution of the parameter of the likelihood of the data.

Gamma will be the posterior with the gamma conjugate prior to generate 
the mean for the distribution

Keep updating conjugate prior with new posterior hyperparameters 
using the gamma distribution 

X ~ Poisson(lambda)

Likihood(x|lambda) ~ Poisson(lambda) <- Data generator
Conjugate Prior - Pi(lambda) ~ Gamma( alpha_0, beta_0) <- Hyper Parameters

lambda is a draw from the conjugate prior

Postierer parameters -> Pi(lambda | x) proportional_to  
Gamma(sum(x_i) + alpha_0, ( n + (1/beta_0) ) -1 )

2. Advanced Example

******************

Bayes using Beta Click through

******************

Objectives:

1- Describe what each of the three parts of the Bayesian learning framework represent
2- Define what a conjugate prior is and why it is useful
3- Describe why the beta-binaormial conjugate pair / likelihood is useful in online learning
4- Define A/B testing
 
Prior: Beta( alpha, beta) for c - click through rate/conversion rate

Likelihood: Binomial( n, c ) = ( n k ) * c **k (1 - c) ** (n - k)  n is number of trials - k is number of successes

e.g. n is number of trials and k is number of click successes e.g. conversions

Postier: New parameters: alpha = alpha + k 
                         beta = beta - k + n

Beta(alpha, beta)  = c ** ( alpha - 1) * ( 1 - c ) ** (beta - 1) / ( beta(alpha, beta) )
Likelihood: Binomial( n, c ) = ( n k ) * c **k (1 - c) ** (n - k)  n is number of trials - k is number of successes

likelihood*prior -> proportional to 

Start with alpha = beta = 1 for uniform prior

first visit is 1 success

alpha = 1 + 1 = 2
beta  = 1 + 1 -1 = 1

next visit is 0 success

alpha = 2 + 0 = 2
beta  = 1 + 1 - 0 = 2

alpha = 3
beta = 2

a = 3 b = 3 or 50% success

Bayes using Beta Click through

********************************

REPO: Multi-armed-bandit

********************************

maximize returns on a slot machine

try and explore multiple slot machines

trade off between exploration and explotation

regret is a meaure of how often you choose a sub-optimal bandit
epsilon first is some exploration and more exploitation

bernoulli trial is yes/no on return beta distributions
baysian update process

start with prior of uniform distribution

Baysian does not have confidence and hyposisis testing 
Frequentist has 95% confidence interval and hyposisis testing 

Frequentists is looking for data that follow a "true" parameters
Prob(Data given parameters)
Baysian is looking "true" parameters givne data
Prob( parameters given data)

######################################################################

WEEK 3: REGRESSION

######################################################################

REPO: Linear Algebra, EDA, Linear Regression

********************************

dot product of two vectors is a scalor or the covariance

euclidean(a ,b) is the distance between two points or vectors

vector norm is a number

cosine_similarity can be used instead of
euclidean(a ,b)

np.corrcoef(a,b)[0,1]

vector_norm(a) - length of the vector from the origin

also called lasso penalty

np.sum([np.absolute(x) for x in a])
np.sum([ x** 2 for x in a])

v = np.random.randint(10, size=4)
v
w = np.random.randint(10, size=4)
w

A = np.random.randint(10, size = 8).reshape(2,4)
A.transpose()

Matrix multiplication is the Dot product. 

mat1 = np.random.randn(12).reshape(4, 3)
mat2 = np.random.randn(15).reshape(3, 5)

a = np.arrange(1,7).reshape(3,2)
b = a.T

in general: A r1 xc1 B r2 xc2

c1 = r2

A dot B = r x c2

A = [[ 1, 2 ],
     [3 , 4], 
     [5 , 6]] 

A.T = [[ 1, 3, 5 ],
       [ 2, 4, 6 ]]

A dot A.t = [[5, 11, 17],
       [ 11, 25, 39 ] ,
       [ 17, 39, 61 ]]

1*2  + 

1*5 + 6* 2 = 17

Inverse of a matrix needs to be a square matrix.

such that the dot product of A and A -1 is equal to the identity matrix

A prime = A transpose

Zero vector = a vector of zeros

Norm is 0 of a zero vector

The vector {3, 4} norm is 5. sqrt( 3**2 + 4**2 ) = 5

The vector {1, 1, 1, 1} = sqrt ( 1 + 1 + 1 + 1 ) = 2

Shape of A 1xn DOT B 1xn  shape 1 x n
A (T) 5X4 shape 4X5

A 10 X 3 dot B 3 X 5 shape 10 X 5

A 3 X 10 dot B 5 X 3 CANNOT BE DONE

(A 5x4 dot B 4X3 ) -1 CANNOT BE DONE

Given A 5X4 , ( A T A ) -1 

A T is 4X5

(4X5 dot 5X4) -1 = > (4 X 4) square so can take inverse

inverse of a (4X4) is (4X4)

Beta = (X tranpose dot X ) -1 dot X transpose y

Dot product of 2 products is a scalor

********************************

REPO: Linear REGRESSION 2

********************************

y = beta0 + beta1(treatment) + E + beta2(age)

look at difference of means between beta0 and beta1 for the treatment
using t test

beta0 is the y interecept 
beta1 is the y interecept 

same data with notation for whether or not treatment is done

BP TR AGE

100 0 25
110 1 30
120 0 40

for regression the distribution of y is contingent on x

Instead of looking at:

y ~ N( mean, deviation)

Linear regression tests for:

y ~ N( x * beta, omega * T)

beta is the slope of the regression line

Regression is used for:

1. Inference

look at coefficents and distributions and check the p value for the mean
for significance

2. Predictions y hat values

RSS is the residual of the sum of squared errors

MSE = 1/N * RSS which is the root mean square error

FMSE = sqrt(MSE) which puts us back on scale

RSE = sqrt( 1 / (n-p-1) * RSS )

residual standard error which accounts for the degrees of freedom

R**2 is a good way to compare different models with different
measurements

R**2 = (TSS - RSS)/ TSS = 1 - RSS/TSS

TSS = summation ( y_i - y_bar) ** 2
RSS = summation ( y_i - y_hat) ** 2

y_bar = mean(y)
y_hat = corresponding y value on estimated line

********************************

REPO: Regularized REGRESSION

Lasso and Ridge Validation

********************************

Robust Residual 

RSS = sum(abs(y - y_hat))

iid = Independent and identically distributed 

errors are iid normally distributed

Studentized residuals

E_i iid N(0, sigma**2)

Need to test normality of this error

Can check:

********************************

REPO: Linear REGRESSION 2
lecture.pdf

********************************

- Normal Q-Q or QQ plot, histogram
Check for normal distribution using QQ plot

Plot studentized residuals to see if they are normal 
against theoretical quantiles and should fall on a line

line up sample data with values in a normal curve
n + 1 buckets mapped on normal distribution.
Order values from lowest to hightest and scatter plot
against normal distribution.

import scipy.stats as scs
import matplotlib.pyplot as plt

y_values = sorted([4.75, 7.90, 7.21, 5.80, 6.33, 5.78, 5.20, 4.75, 3.89])
n = len(y_values)
z_values =  [[scs.norm.ppf(float(i)/(n+1)) for i in xrange(1,n+1) ]]

plt.scatter(z_values, y_values)
plt.show()

Studentized Residuals should be N(0,1)

Local regression - use sliding weight function, make separate linear fits over range X

Generalized additive model - just add up contributing effects

Need to check for normal error_i  is a normal distribution
it is proportional to studentized residials ~ N(0,1)

Need to check that the error rate is sigma**2

Solution is to look at y - because it is used to calculate error
adding more components will not help.

****************************************************************

Multiple Linear Regression:

More x's available to predict y

1) Constant Variance (homoscedasticity)
2) Linear model is approriate
3) Normally distribution of the errors 
4) iid  - Indenpendent and identically distributed
5) no multicollineartity - or coorelation between any variables

multicollinearity - 

Can check by VIF variance inflation factors

1/ (1 - R**2)
Large values indicate covariance

- Correlation Matrix / Scatterprlot Matrix

- Variance Inflation Factors (VIF)

-- Run ordinary least squares for each predictor as function of 
all the other predictors . k times for f predictors

Rule of Thumb > 10 is problematic

****************************************************************

OUTLIERS:

occur when y_i is far from predicted y_i_hat (estimated)

find out why it is an outlier before removing

Correlation is a linear relationship between factors

Dividing each residual by its standard error,
should result in a "studentized residual" between -2 and 2
for 2 standard deviations
as a rule of thumb

LEVERAGE

Leverage point : Var(e_i) = (1 - h_ii) * sigma ** 2

Beta = (X.T * X) ** -1 X.t * y

XBeta = y_hat = H * y

H = X ( X.T * X) ** -1 X.T

h_ii = (H)_ii

some points affect the regression more than other points

Most influential points are outliers with high leverage (far from 
cluster of points

Look for interaction between variables by seeing systematic
under or over predicion of values using heat map

p value should be LESS than 0.05 for regression results

Variance - in terms of fitting a model - fluctuations 
in model due to change in our training data

Bias - error introduced by modeling complex data with
simplistic model - how far off the expected value is from
the true model

Strike a balance between variance and bias

- 100 x variables need to decide on which 20 variables to
use look at p values to see what to remove
after fitting all variables

- look for redundant variables or classified together and 
choose one

- business choice for predictive variables

Coefficent size is in proportion of the x variable so not 
relative to each other

**************

BEST SUBSET

**************

Fit all possible models - computation intensive so not
practical

**************

FORWARD SELECTION

**************

Fit it with each variable alone
Pick winner with highest RSS

Pick winner with combo of the rest of the variables

**************

BACKWARD SELECTION

**************

Fit with all variables
Remove on variable at a time and compare results

**************

Drawbacks of R**2 / RSS and why measures like
AIC, BIC, and adjusted R**2 are more approriate.

R**2 is easy to understand

R**2 is localized to the training data so it
is not reflective of what it may be like with
a new data set

MEDIOCRE TESTS:

R**2 RSS

R**2 increases or stays the same
with more variables and RSS goes down or stays the
same

BETTER TESTS:

Adjusted R**2 is better - higher is better = 1 - RSS/(n-p-1)
                                                ____________
                                                 TSS/(n-1)
AIC - lower is better = -2ln(L) + 2k
BIC - lower is better = -2ln(L) + ln(n/k)

Introduce penalty term for complexity

k - number of predictors
n - sample size or number of observations
L - likelihood
p - number of predictors

BEST TEST:

MSS on a cross validation data set

******************************************************

TEST FINAL MODEL FOR BEST MODEL COMPARISON
USE VALIDATION

******************************************************

*********************

Validation Set Approach
----------> Not a good approach

all data is used for generating models to be
compared against each other

1. Fit model on training set
2. Use fitted model in 1 to predict responses for validation set
3. Compute validation-set error

*********************

Hold out data for second test for new error metric on a new
data set.

Validation Set Approach:

randomly pull training and validation set from the entire
data set

Validation training set is used to calculate the MSE

Randomly keep pulling sets out of the data and recalculate
MSE 

*********************

Cross Validation Set Approach

test set < 50% - model never sees it 

training set is used for cross validation
never used for creating the model

Don't take random subsets
Take a predetermined subsets

*********************
Leave one out cross validation
*********************

For each model:

    1 - fit same model n times
    2 - method 
     for n times within the training set:

         a - take n - 1 data points
         b - fit the model on n - 1 data points
         c - predict the n'th data point that was left out
         d - calculate error
        
    3 - MSE of leave one out cross validation

*********************
k-fold validation

----------> Best validation

*********************

Pick the number of folds between 5 and 10 not at random

1. Fit model on training set, using (k-1) folds
2. Use fitted model in 1 to predict responses for 
validation set, 1 of the folds
3. Compute validation-set error

For each model:

    1 - fit same model k fold times (training set divided k times)
    2 - method 
     for k times within the training set with the folds:

         a - leave out k fold for training
         b - fit the model on the remaining folds
         c - predict the fold that was left out
         d - calculate error
        
    3 - MSE of k fold cross validation

******************************************************

Comparison of models

BIAS

LOOCV < k-fold < validation set

VARIANCE 

validations set < k-fold < LOOCV

******************************************************

After training and validating data then use all the
data before deploying model using training and test 
set data to get the final parameters

******************************************************

Bias-Variance Tradeoff

******************************************************

Variance is high complexity . Amount by which f_hat 
would changed if estimated it using a different training
dataset. Often overfitting problem.

Bias difference between expected prediction of our
model and the correct value we are trying to predict
Measure of the prediction of the correct value.

******************************************************
Regularization Ridge Regression

lambda is a tuning parameter

Regularization Lasso Regression

when lambda = 0 we have linear models

lambda is a tuning parameter

L1 is the least absolute values
L2 is the least squares

Standardize the predictors by dividing by the
standard deviation

Need to do this anytime that you need to 
penalize beta

********************************

REPO: Assessment-3 
      Logistic =-regression

********************************

Motivating problem:

[Binary] Claassification

Response Y is an element of {0, 1}

which is mutually exclusive

Goals:

1 - Estimate
2 - Interpret
3 - Predict
4 - Metrics

Preliminary:

What Distribution?

Bernolli takes p parameter

PMF:

Expected Value ( y ) = p if y = 1
                     1-p if y = 0

LIKELIHOOD function: or expected value

E(y) = p**y ( 1 - p) ** (1-y)

Variance:

p

OLS is a straight line

which is not a good fit for logistic regression 

OLS does deal well with separation

separation is the strength of a factor in the regression

Variance of OLS

errors are contigent on x
x *beta ( 1 - x *beta) 
if x is continuous p becomes x and becomes continuous

errors are heteroscatic which is a violation of normality

Need link function for logistic regression:

1 / ( 1 + exp(-X beta ))

Likelihood for all the observations in the data 

First step to maximize parameters is to change the products to
logs.

Instead work with log liklihood and take the natural log:

which turns the product into a sum of values.

Log likelihood of values between 0 and 1 (probablilty) is a
negative number. so take the negative value.

Penalty term

L1 = sum of absolute values of beta 
L2 = sum of beta squared

Penalize likelihood by subtracting penalty term

lnL = blabla_summing - [L1, L2]

Biased coin:

P(H) = (0.75)
P(T) = (1.0 - 0.75) = 0.25

Probability event for odds of heads:

0.75/ 0.25 = 3 

or 3 to 1

ODDS are :

p / (1-p)

take log of this value will yield :

X * beta

Get back odds from skilear by :

exp( beta )

predicted probabilities 

********************************

LOGISTIC METRICS

********************************

Need to classify predicted events as a classified event
0 or 1 using a threshold. Default is 0.5 . Above 0.5 is 
classified as an event otherwise not an event (zero).

True positives are above the threshold

Lower Thresholds have more false positive Type I error
Higher Thresholds have more false negatives Type II error

True negative are below the threshold

Partitioning into quardrents is called a confusion 
matrix to show what is correct and what is not.

1. Accuracy = skilearn score = all correct / all cases

2. True Positive Rate = TPR = true_positives / over positive cases
Recall or sensitiviy

3. False Positive Rate = FPR = false_positives / N
1 - specifity

4. True Negative Rate = TPR = true_negatives / over negative cases
specifity

5. Precision = true positives / (true positives + false positives)

Increasing the threshold will:

:grinning: decrease the number of False Positives,
:weary: decrease the number of True Positives,
:weary: increase the number of False Negatives, and
:grinning: increase the number of True Negatives.

ROC:

function ROC_curve(probabilities, labels):
    Sort instances by their prediction strength (the probabilities)
    For every instance in increasing order of probability:
        Set the threshold to be the probability
        Set everything above the threshold to the positive class
        Calculate the True Positive Rate (aka sensitivity or recall)
        Calculate the False Positive Rate (1 - specificity)

Area under ROC should be closest to 1 for ideal values

ROC curve should not below diagnol line

Accuracy may be a bad example - 

********************************

REPO: GRADIANT-DESCENT

********************************

What: Numerical Optimization

Why: minimize a a cost function such as MSE (descent)
or maximize a log likelihood function (ascent)

e.g.  E(y) = p**y ( 1 - p) ** (1-y)

pi = 1 / ( 1 + e ** -x*beta)

Gradient is the partial derivative. Positive slope is going up
Negative slope is going down.

Zero value is where the slope changes direction

hessian is the second derivative and is reflective of the 
standard error. 

Tighter curves have smaller hessians than wider curves

Step size of riding the curve is called "alpha"

while lnL increasing:

1/beta = 1/beta + ( alpha * gradient )

Set a threshhold for the value close enough to zero.

Option Criteria:

1) derivative of beta ( ln L ) ~ 0
2) cap for number of iterations (max_iter)

Two Types of Gradient Ascent:

1) Batch Gradient Ascent:

set beta_k = 0

while lnL increasing on full training data:

beta = beta + ( alpha * gradient)

2) Stochastic Gradient Descent:

set beta_k = 0

row by row
while ln L increasing:
    for i to N:
        beta = beta + ( alpha * G)

Regression Review:

Simple Linear Regression
Assumptions

y ~ N(x Beta , sigma ** 2)

y_hat = beta_0 + beta_1 * x_1

1) Constant Variance
2) Linear model is approriate
3) Normally distribution of the errors 
4) iid  - Indenpendent and identically distributed

one unit in change with beta coefficient changes y by coefficient
in regression holding everything else constant.

Add dummy variables for categorical variables in regression
Add levels - 1 for number of dummy variables.

Diagnostics - compare linear regression models - MSE , AIC,  adjusted R**2, BIC

Check residual plot for residuals vs. independent variables

k folds to compare models with predictive power

Inapproriate measures - MSE of training set the one with more variables with have a better MSE

######################################################################

WEEK 4 SUPERVISED LEARNING

######################################################################

REPO: Non Parametric Learners

********************************

Generalized Linear Models (GLMs) are parametric
Regression is parametric with normal assumptions
Response values - continuous (linear regression) vs. binary for logistic
Supervised - has predictive label (linear and logistic regression) vs 
Unsupervised - no predictive labels  - looking at relationships
Parameters of Interest
Predictor values
Bias-variance trade off - linear model high bias and low variance 
Bias-variance trade off - decision trees have model low bias and 
high variance 
Advantage of Regression is that it has interpretable beta coefficients

recursion. 

The 3 laws of recursion algorithm:

1. must have a base case.
2. must change its state and move toward the base case.
3. must call itself, recursively.

k-nearest neighbors (kNN) - high bias approach

Parameters 1- how many neighbors 2- neighbors weight

k should be an odd number so no ties

Low neighbors (k) has low bias and high variance
High neighbors (k) as high bias and low variance

Library sklearn.neighbors.KNeighborsClassifier

Response Values - Mostly uses categorical values
Predictor Values - Categorical or Numerical

Measures of "closeness" :
- Cosine similarity - e.g. good with text data for scaling from 3 to 300
because on the same line (3,4) is on the same line as (300, 400)
- Euclidean Distance - e.g. not good with scaling (3,4) is not close to
(300, 400)

Pros: High accuracy, insensitive to outliers, no assumptions about data
Cons: Computationally expensive, requires a lot of memory
Works with: Numeric values, nominal values

1. Collect: Any method.
2. Prepare: Numeric values are needed for a distance calculation. 
A structured data format is best.
3. Analyze: Any method.
4. Train: Does not apply to the kNN algorithm.
5. Test: Calculate the error rate.
6. Use: This application needs to get some input data and output structured 
numeric values. Next, the application runs the kNN algorithm on this 
input data and determines which class the input data should belong to. 
The application then takes some action on the calculated class.

For every point in our dataset:
calculate the distance between inX and the current point
sort the distances in increasing order
take k items with lowest distances to inX
find the majority class among these items
return the majority class as our prediction for the class of inX

Decision Trees

Pros: Computationally cheap to use, easy for humans to understand 
learned results, missing values OK, can deal with irrelevant features
Cons: Prone to overfitting
Works with: Numeric values, nominal values


1. Collect: Any method.
2. Prepare: This tree-building algorithm works only on nominal values, 
so any continuous values will need to be quantized.
3. Analyze: Any method. You should visually inspect the tree 
after it is built.
4. Train: Construct a tree data structure.
5. Test: Calculate the error rate with the learned tree.
6. Use: This can be used in any supervised learning task. 
Often, trees are used to better understand the data.

ID3 algorithm

http://en.wikipedia.org/wiki/ID3 _algorithm

Using information theory, you can measure the information before and 
after the split.

The measure of information of a set is known as the Shannon entropy, 
or just entropy for short.

To calculate entropy, you need the expected value of all the information 
of all possible values of our class.

Another common measure of disorder in a set is the Gini impurity.

Assumptions about data for trees.py:

1) Data comes in the form of a list of lists 
2) all these lists are of equal size 
3) last column in the data or the last item in each instance is the 
class label of that instance 

You use these assumptions in the first line of the function to find 
out how many features you have available 
in the given dataset. 

We didn’t make any assumption on the type of data 
in the lists. It could be a number or a string; it doesn’t matter.

Decision trees can be applied to both regression and classification problems

REGRESSION

Building a Regression Tree
1. Use recursive binary splitting to grow a large tree on the training
data, stopping only when each terminal node has fewer than some
minimum number of observations.
2. Apply cost complexity pruning to the large tree in order to obtain a
sequence of best subtrees, as a function of α .
3. Use K-fold cross-validation to choose α . That is, divide the training
observations into K folds. For each k =1 ,...,K :
(a) Repeat Steps 1 and 2 on all but the k th fold of the training data.
(b) Evaluate the mean squared prediction error on the data in the
left-out k th fold, as a function of α .

Average the results for each value of α , and pick α to minimize the
average error.
4. Return the subtree from Step 2 that corresponds to the chosen value

********************************

REPO: Random Forest

********************************

CLASSIFICATION

A classification tree is very similar to a regression tree, 
except that it is classification tree
used to predict a qualitative response rather than a quantitative one.

We predict that each observation belongs to the
most commonly occurring class of training
observations in the region to which it belongs.

class proportions among the training observations that fall into that region

Decision trees for regression and classification have a number of 
advantages over the more classical approaches such as regression:

1) Trees are very easier to explain than linear regression

2) decision trees more closely mirror human decision-making 
than do the regression and classification approaches

3) Trees can be displayed graphically, and are easily interpreted 

4) Trees can easily handle qualitative predictors without the need to
create dummy variables.

Disadvantage

---> Not good predictors 
---> High variance

Bagging, random forests, and boosting use trees as building blocks to
construct more powerful prediction models 

combining a large number of trees can often result in dramatic
improvements in prediction accuracy, at the expense of some 
loss in interpretation.

bagging - bootstrap aggregation - averaging a set of observations 
reduces variance

samples from the (single) training data set. In this approach we generate 
B different bootstrapped training data sets. 
We train our method on the b th bootstrapped training set:

f_hat ∗ b (x), and finally average all the predictions, to obtain

f_hat_bag (x)= 1 B B b =1 f_hat ∗ b (x) This is called bagging.

random forests - in building a random forest, at each split in the tree,
the algorithm is not even allowed to consider a majority of the available
predictors.

The main difference between bagging and random forests is the choice
of predictor subset size m .

boosting

Boosting works like bagging, except that the trees are
grown sequentially : each tree is grown using information from previously
grown trees. Boosting does not involve bootstrap sampling; instead each
tree is fit on a modified version of the original data set

Boosting has three tuning parameters:

1. The number of trees B . Unlike bagging and random forests, boosting
can overfit if B is too large, although this overfitting tends to occur
slowly if at all. We use cross-validation to select B .

2. The shrinkage parameter λ , a small positive number. This controls the
rate at which boosting learns. Typical values are 0 .  01 or 0 .  001, and
the right choice can depend on the problem. Very small λ
can require using a very large value of B in order to achieve good performance.

3. The number d of splits in each tree, which controls the complexity
of the boosted ensemble. Often d = 1 works well, in which case each
tree is a stump , consisting of a single split. In this case, the boosted
stump ensemble is fitting an additive model, since each term involves only a
single variable. More generally d is the interaction depth ,and controls
interaction depth the interaction order of the boosted model, since
d splits can involve at most d variables.

********************************

REPO: SVM

********************************

The support vector machine (SVM) is a generalization of a simple and 
intuitive classifier called the maximal margin classifier.

Hyperplane - In a p-dimensional space, a hyperplane
is a flat affine subspace of hyperplane dimension p − 1.

A natural choice is the maximal margin hyperplane
(also known as the maximal margin hyperplane optimal separating hyperplane
), which is the separating hyperplane that optimal separating hyperplane
is farthest from the training observations

Advantages:

1) Greater robustness to individual observations
2) Better classification of most of the training observations.

The support vector machine (SVM) is an extension of the support vector
support vector machine classifier that results from enlarging the 
feature space in a specific way, using kernels .

********************************

REPO: Boosting

********************************



********************************

REPO: Assessment-4
      Profit-Curves

********************************





